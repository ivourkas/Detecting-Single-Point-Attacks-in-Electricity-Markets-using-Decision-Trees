{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687d68d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 CPU cores for parallel processing\n",
      "\n",
      "=== Loading and preprocessing data ===\n",
      "Dataset loaded with shape: (1920, 34036)\n",
      "Total records: 1920\n",
      "Number of attacks: 960\n",
      "Number of normal operations: 960\n",
      "Attack percentage: 50.00%\n",
      "\n",
      "Attack type distribution:\n",
      "type_of_attack\n",
      "3    276\n",
      "1    240\n",
      "2    236\n",
      "4    208\n",
      "Name: count, dtype: Int64\n",
      "Lower Limit Attack: 276 instances\n",
      "Ramp Rate Attack: 240 instances\n",
      "Upper Limit Attack: 236 instances\n",
      "Generation Cost Attack: 208 instances\n",
      "\n",
      "Top 10 attacked generators:\n",
      "gen_attacked\n",
      "25    40\n",
      "60    36\n",
      "19    28\n",
      "39    28\n",
      "52    28\n",
      "12    28\n",
      "29    28\n",
      "61    28\n",
      "23    28\n",
      "16    24\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "=== Performing domain-specific feature engineering ===\n",
      "\n",
      "=== Performing advanced feature engineering ===\n",
      "\n",
      "1. Creating specialized Lower Limit Attack detection features...\n",
      "3. Creating features for large changes without ramp constraint activation...\n",
      "4. Creating temporal pattern detection features...\n",
      "5. Creating composite attack-specific features...\n",
      "6. Creating interaction features with new composite features...\n",
      "Created 18 new interaction features\n",
      "\n",
      "Advanced feature engineering complete. Added 65 new features.\n",
      "New DataFrame shape: (1920, 34137)\n",
      "\n",
      "=== Preparing features for modeling ===\n",
      "Training set shape: (1536, 34134)\n",
      "Test set shape: (384, 34134)\n",
      "\n",
      "Applying variance threshold...\n",
      "Features after variance thresholding: 18397\n",
      "\n",
      "Selecting most informative features...\n",
      "Features after selection: 100\n",
      "\n",
      "Top 20 most informative features:\n",
      "ft4331: 0.040645\n",
      "ft4086: 0.038516\n",
      "ft1322: 0.036822\n",
      "ft6887: 0.032083\n",
      "ft5890: 0.029119\n",
      "ft6632: 0.028593\n",
      "ft165: 0.027925\n",
      "ft257: 0.026656\n",
      "ft2633: 0.026624\n",
      "ft77: 0.026397\n",
      "ft3042: 0.026175\n",
      "ft3796: 0.025748\n",
      "ft8412: 0.025328\n",
      "ft9314: 0.025020\n",
      "ft7003: 0.024198\n",
      "ft1683: 0.023655\n",
      "ft3054: 0.023264\n",
      "ft7193: 0.022503\n",
      "ft7048: 0.022395\n",
      "ft4446: 0.022134\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "=== Training and evaluating models ===\n",
      "\n",
      "1. Training Extra Trees Classifier\n",
      "Best parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 30, 'bootstrap': False}\n",
      "\n",
      "Extra Trees Results:\n",
      "Accuracy: 0.9974\n",
      "F1 Score: 0.9974\n",
      "Confusion Matrix:\n",
      "[[189   1]\n",
      " [  0 194]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       190\n",
      "           1       0.99      1.00      1.00       194\n",
      "\n",
      "    accuracy                           1.00       384\n",
      "   macro avg       1.00      1.00      1.00       384\n",
      "weighted avg       1.00      1.00      1.00       384\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_cos_10d: 0.147326\n",
      "ramp_constraint_active: 0.084302\n",
      "fval_cos_10d_interaction: 0.059056\n",
      "large_change_no_constraint_score: 0.057707\n",
      "fval_sin_20d: 0.056862\n",
      "large_change_no_constraint_magnitude: 0.050991\n",
      "fval_cos_20d: 0.042345\n",
      "fval_change: 0.034300\n",
      "day_in_ssn_x_large_change_no_constraint_score: 0.031798\n",
      "lower_limit_approach_rate: 0.028205\n",
      "\n",
      "2. Training Gradient Boosting Classifier\n",
      "Best parameters: {'subsample': 0.8, 'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[190   0]\n",
      " [  0 194]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       190\n",
      "           1       1.00      1.00      1.00       194\n",
      "\n",
      "    accuracy                           1.00       384\n",
      "   macro avg       1.00      1.00      1.00       384\n",
      "weighted avg       1.00      1.00      1.00       384\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_cos_10d: 0.223501\n",
      "fval_diff_lag_1: 0.216859\n",
      "lower_limit_approach_rate: 0.178131\n",
      "fval_cos_10d_interaction: 0.084275\n",
      "fval_diff_lag_1_x_lower_limit_attack_score: 0.059106\n",
      "fval_diff_lag_1_x_weighted_attack_score: 0.047702\n",
      "fval_change_jerk: 0.039999\n",
      "fval_change: 0.039552\n",
      "fval_change_x_weighted_attack_score: 0.036599\n",
      "fval_change_x_lower_limit_attack_score: 0.034117\n",
      "\n",
      "3. Training Random Forest Classifier\n",
      "Best parameters: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 20, 'bootstrap': False}\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.9818\n",
      "F1 Score: 0.9823\n",
      "Confusion Matrix:\n",
      "[[183   7]\n",
      " [  0 194]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       190\n",
      "           1       0.97      1.00      0.98       194\n",
      "\n",
      "    accuracy                           0.98       384\n",
      "   macro avg       0.98      0.98      0.98       384\n",
      "weighted avg       0.98      0.98      0.98       384\n",
      "\n",
      "ROC AUC: 0.9961\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_diff_lag_1_x_lower_limit_attack_score: 0.074950\n",
      "fval_change_x_weighted_attack_score: 0.073353\n",
      "lower_limit_approach_rate: 0.067356\n",
      "fval_diff_lag_1: 0.065474\n",
      "fval_change: 0.064708\n",
      "fval_diff_lag_1_x_weighted_attack_score: 0.064349\n",
      "fval_cos_10d: 0.055123\n",
      "fval_change_x_lower_limit_attack_score: 0.048341\n",
      "fval_cos_10d_interaction: 0.030766\n",
      "fval_change_jerk: 0.029322\n",
      "\n",
      "4. Training XGBoost Classifier\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.8}\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[190   0]\n",
      " [  0 194]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       190\n",
      "           1       1.00      1.00      1.00       194\n",
      "\n",
      "    accuracy                           1.00       384\n",
      "   macro avg       1.00      1.00      1.00       384\n",
      "weighted avg       1.00      1.00      1.00       384\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_diff_lag_1_x_weighted_attack_score: 0.342313\n",
      "fval_diff_lag_1: 0.190810\n",
      "fval_change_x_lower_limit_attack_score: 0.144725\n",
      "lower_limit_approach_rate: 0.141895\n",
      "fval_cos_10d: 0.029780\n",
      "fval_change_std_3d_x_large_change_no_constraint_score: 0.025438\n",
      "fval_ar5_residual: 0.021341\n",
      "fval_change_jerk: 0.018205\n",
      "fval_cos_10d_interaction: 0.012399\n",
      "fval_change_std_3d: 0.012166\n",
      "\n",
      "5. Training Logistic Regression\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 2000, 'C': 1000.0}\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.8698\n",
      "F1 Score: 0.8792\n",
      "Confusion Matrix:\n",
      "[[152  38]\n",
      " [ 12 182]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86       190\n",
      "           1       0.83      0.94      0.88       194\n",
      "\n",
      "    accuracy                           0.87       384\n",
      "   macro avg       0.88      0.87      0.87       384\n",
      "weighted avg       0.88      0.87      0.87       384\n",
      "\n",
      "ROC AUC: 0.9280\n",
      "\n",
      "6. Training Voting Classifier Ensemble\n",
      "Top 3 models for ensemble: ['GradientBoosting', 'XGBoost', 'ExtraTrees']\n",
      "\n",
      "Voting Classifier Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[190   0]\n",
      " [  0 194]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       190\n",
      "           1       1.00      1.00      1.00       194\n",
      "\n",
      "    accuracy                           1.00       384\n",
      "   macro avg       1.00      1.00      1.00       384\n",
      "weighted avg       1.00      1.00      1.00       384\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "=== Model Performance Summary ===\n",
      "\n",
      "Models ranked by F1 score:\n",
      "1. Gradient Boosting: Accuracy = 1.0000, F1 = 1.0000\n",
      "2. XGBoost: Accuracy = 1.0000, F1 = 1.0000\n",
      "3. Voting Classifier: Accuracy = 1.0000, F1 = 1.0000\n",
      "4. Extra Trees: Accuracy = 0.9974, F1 = 0.9974\n",
      "5. Random Forest: Accuracy = 0.9818, F1 = 0.9823\n",
      "6. Logistic Regression: Accuracy = 0.8698, F1 = 0.8792\n",
      "\n",
      "Best model (Gradient Boosting) saved as 'best_model_Gradient_Boosting.pkl'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_curve, auc\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the number of cores for parallel processing\n",
    "n_cores = max(1, os.cpu_count() // 2)\n",
    "print(f\"Using {n_cores} CPU cores for parallel processing\")\n",
    "\n",
    "# ====================== LOAD AND PREPROCESS DATA ======================\n",
    "print(\"\\n=== Loading and preprocessing data ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/N300_G69_transposed.csv\", dtype={'ssn':'Int64', 'type_of_attack': 'Int64', 'gen_attacked': 'Int64'})\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "\n",
    "# Create binary target column if not already present\n",
    "if 'attack' not in df.columns:\n",
    "    df['attack'] = (df['type_of_attack'] != 0).astype(int)  # 1 for attack, 0 for no attack\n",
    "\n",
    "# Display class distribution\n",
    "attack_count = df['attack'].sum()\n",
    "total_records = len(df)\n",
    "normal_count = total_records - attack_count\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Number of attacks: {attack_count}\")\n",
    "print(f\"Number of normal operations: {normal_count}\")\n",
    "print(f\"Attack percentage: {attack_count/total_records:.2%}\")\n",
    "\n",
    "# Distribution of attack types\n",
    "if 'type_of_attack' in df.columns:\n",
    "    attack_types = df[df['attack'] == 1]['type_of_attack'].value_counts()\n",
    "    print(\"\\nAttack type distribution:\")\n",
    "    print(attack_types)\n",
    "    \n",
    "    # Map attack types to descriptive names\n",
    "    attack_type_names = {\n",
    "        0: \"Normal\",\n",
    "        1: \"Ramp Rate Attack\",\n",
    "        2: \"Upper Limit Attack\",\n",
    "        3: \"Lower Limit Attack\",\n",
    "        4: \"Generation Cost Attack\"\n",
    "    }\n",
    "    \n",
    "    # Print distribution with names\n",
    "    for attack_type, count in attack_types.items():\n",
    "        print(f\"{attack_type_names.get(attack_type, 'Unknown')}: {count} instances\")\n",
    "\n",
    "# Distribution of attacked generators\n",
    "if 'gen_attacked' in df.columns:\n",
    "    attacked_gens = df[df['attack'] == 1]['gen_attacked'].value_counts()\n",
    "    print(\"\\nTop 10 attacked generators:\")\n",
    "    print(attacked_gens.head(10))\n",
    "\n",
    "# ====================== DOMAIN-SPECIFIC FEATURE ENGINEERING ======================\n",
    "print(\"\\n=== Performing domain-specific feature engineering ===\")\n",
    "\n",
    "# 1. Basic session and time features\n",
    "df['day_in_ssn'] = df.groupby('ssn').cumcount() + 1  # Start at Day 1\n",
    "\n",
    "# 2. Calculate OPF sensitivity features\n",
    "# These help identify attacks on specific OPF parameters\n",
    "\n",
    "# 2.1 Cost function sensitivity indicators\n",
    "# Create features that might detect manipulation of generation costs (Type 4 attack)\n",
    "df['fval_per_unit_load'] = df['fval'] / df.groupby('ssn')['fval'].transform('mean')\n",
    "df['fval_normalized_by_ssn'] = df.groupby('ssn')['fval'].transform(\n",
    "    lambda x: (x - x.mean()) / (x.std() if x.std() != 0 else 1))\n",
    "\n",
    "# 2.2 Ramp rate indicators \n",
    "# Create features that might detect manipulation of ramp rates (Type 1 attack)\n",
    "df['fval_change'] = df.groupby('ssn')['fval'].diff().fillna(0)\n",
    "df['fval_change_rate'] = df['fval_change'] / df['fval'].shift(1).fillna(1)\n",
    "df['fval_acceleration'] = df.groupby('ssn')['fval_change'].diff().fillna(0)\n",
    "\n",
    "# Create rolling metrics to detect unusual ramp behavior\n",
    "for window in [2, 3, 5]:\n",
    "    # Rolling standard deviation of changes (volatility)\n",
    "    df[f'fval_change_std_{window}d'] = df.groupby('ssn')['fval_change'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).std().fillna(0))\n",
    "    \n",
    "    # Maximum change in the window\n",
    "    df[f'fval_max_change_{window}d'] = df.groupby('ssn')['fval_change'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).max().fillna(0))\n",
    "    \n",
    "    # Minimum change in the window\n",
    "    df[f'fval_min_change_{window}d'] = df.groupby('ssn')['fval_change'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).min().fillna(0))\n",
    "\n",
    "# 2.3 Generation limit indicators\n",
    "# Create features that might detect manipulation of upper/lower limits (Type 2 & 3 attacks)\n",
    "df['fval_peak_ratio'] = df['fval'] / df.groupby('ssn')['fval'].transform('max')\n",
    "df['fval_trough_ratio'] = df['fval'] / df.groupby('ssn')['fval'].transform('min')\n",
    "\n",
    "# Use quantiles to detect limits being approached\n",
    "df['fval_quantile_in_ssn'] = df.groupby('ssn')['fval'].transform(\n",
    "    lambda x: pd.qcut(x, q=10, labels=False, duplicates='drop').astype(float))\n",
    "\n",
    "# 3. Create specialized detection features for each attack type\n",
    "# These target the specific mechanisms of each attack type\n",
    "\n",
    "# 3.1 Features for detecting ramp rate attacks (Type 1)\n",
    "# Look for sudden discontinuities in how fast fval can change\n",
    "df['ramp_constraint_active'] = (df['fval_change'].abs() > \n",
    "                               df.groupby('ssn')['fval_change'].transform('std')).astype(int)\n",
    "\n",
    "# 3.2 Features for detecting generation limit attacks (Types 2 & 3)\n",
    "# Look for values that should be infeasible under normal limits\n",
    "df['upper_limit_proximity'] = 1 - (df['fval'] / df.groupby('ssn')['fval'].transform('max'))\n",
    "df['lower_limit_proximity'] = (df['fval'] / df.groupby('ssn')['fval'].transform('min')) - 1\n",
    "\n",
    "# 3.3 Features for detecting cost manipulation (Type 4)\n",
    "# Look for cost-inefficient dispatches that shouldn't happen under normal cost functions\n",
    "df['cost_efficiency'] = df['fval'] / df.groupby('ssn')['fval'].transform('mean')\n",
    "df['cost_anomaly_score'] = df.groupby('ssn')['cost_efficiency'].transform(\n",
    "    lambda x: (x - x.mean()).abs() / (x.std() if x.std() != 0 else 1))\n",
    "\n",
    "# 4. Inter-session comparison features\n",
    "# These help identify if a session is behaving differently from others\n",
    "\n",
    "# 4.1 Calculate average fval across all sessions for each day\n",
    "day_avg = df.groupby('day_in_ssn')['fval'].transform('mean')\n",
    "day_std = df.groupby('day_in_ssn')['fval'].transform('std')\n",
    "\n",
    "# 4.2 Compare each session's values to the average across all sessions\n",
    "df['fval_day_deviation'] = (df['fval'] - day_avg) / (day_std if day_std.any() != 0 else 1)\n",
    "df['fval_day_pct_diff'] = (df['fval'] - day_avg) / day_avg\n",
    "\n",
    "# 5. Statistical anomaly detection features\n",
    "# These help identify outliers regardless of attack mechanism\n",
    "\n",
    "# 5.1 Z-scores and modified Z-scores for more robust outlier detection\n",
    "df['fval_median_dev'] = df.groupby('ssn')['fval'].transform(\n",
    "    lambda x: (x - x.median()) / (x.max() - x.min() if (x.max() - x.min()) != 0 else 1))\n",
    "\n",
    "# 6. Cumulative features to detect subtle long-term manipulations\n",
    "df['fval_cumsum'] = df.groupby('ssn')['fval'].cumsum()\n",
    "df['fval_cummean'] = df.groupby('ssn')['fval'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['fval_cum_deviation'] = df['fval'] - df['fval_cummean']\n",
    "\n",
    "# 7. Trajectory features to detect changes in patterns\n",
    "for lag in range(1, 4):\n",
    "    df[f'fval_lag_{lag}'] = df.groupby('ssn')['fval'].shift(lag).fillna(0)\n",
    "    df[f'fval_diff_lag_{lag}'] = df['fval'] - df[f'fval_lag_{lag}']\n",
    "\n",
    "\n",
    "\n",
    "# Clean any NaN values\n",
    "df = df.fillna(0)\n",
    "\n",
    "# ====================== ADVANCED FEATURE ENGINEERING ======================\n",
    "# Apply the advanced feature engineering \n",
    "from advanced_features import engineer_advanced_features\n",
    "df = engineer_advanced_features(df)\n",
    "\n",
    "# ====================== FEATURE SELECTION AND MODELING ======================\n",
    "print(\"\\n=== Preparing features for modeling ===\")\n",
    "\n",
    "# Define feature columns, excluding the target and direct identifiers\n",
    "feature_columns = [col for col in df.columns if col not in ['attack', 'type_of_attack', 'gen_attacked']]\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df['attack']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Remove low variance features\n",
    "print(\"\\nApplying variance threshold...\")\n",
    "variance_threshold = 0.01\n",
    "selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_train_var = selector.fit_transform(X_train)\n",
    "X_test_var = selector.transform(X_test)\n",
    "\n",
    "print(f\"Features after variance thresholding: {X_train_var.shape[1]}\")\n",
    "\n",
    "# Feature selection using mutual information\n",
    "print(\"\\nSelecting most informative features...\")\n",
    "select_k = SelectKBest(mutual_info_classif, k=min(100, X_train_var.shape[1]))\n",
    "X_train_selected = select_k.fit_transform(X_train_var, y_train)\n",
    "X_test_selected = select_k.transform(X_test_var)\n",
    "\n",
    "print(f\"Features after selection: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# Print top 20 feature names\n",
    "selected_indices = select_k.get_support(indices=True)\n",
    "original_indices = selector.get_support(indices=True)\n",
    "selected_names = [X.columns[original_indices[i]] for i in selected_indices[:20]]\n",
    "scores = select_k.scores_[selected_indices]\n",
    "\n",
    "print(\"\\nTop 20 most informative features:\")\n",
    "for name, score in sorted(zip(selected_names, scores), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{name}: {score:.6f}\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# ====================== TRAIN MULTIPLE MODELS ======================\n",
    "print(\"\\n=== Training and evaluating models ===\")\n",
    "\n",
    "# Define a function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Attack'],\n",
    "                yticklabels=['Normal', 'Attack'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.savefig(f'{model_name.replace(\" \", \"_\")}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC curve and AUC (if applicable)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        try:\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                    label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve - {model_name}')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(f'{model_name.replace(\" \", \"_\")}_roc_curve.png')\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate ROC AUC: {e}\")\n",
    "            \n",
    "    # Feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_names = [X.columns[original_indices[i]] for i in selected_indices]\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Plot top 20 feature importances\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(f'Top 20 Feature Importances - {model_name}')\n",
    "        plt.bar(range(min(20, len(importances))), \n",
    "                importances[indices[:20]], align='center')\n",
    "        plt.xticks(range(min(20, len(importances))), \n",
    "                   [feature_names[i] for i in indices[:20]], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_name.replace(\" \", \"_\")}_feature_importance.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"\\nTop 10 most important features:\")\n",
    "        for i in range(min(10, len(importances))):\n",
    "            print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.6f}\")\n",
    "    \n",
    "    return model, accuracy, f1\n",
    "\n",
    "# ====================== MODEL 1: EXTRA TREES CLASSIFIER ======================\n",
    "print(\"\\n1. Training Extra Trees Classifier\")\n",
    "\n",
    "# Define parameter grid\n",
    "et_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "et_base = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "et_cv = RandomizedSearchCV(\n",
    "    et_base, et_param_grid, n_iter=200, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "et_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "et_best = et_cv.best_estimator_\n",
    "print(f\"Best parameters: {et_cv.best_params_}\")\n",
    "\n",
    "# Evaluate\n",
    "et_model, et_accuracy, et_f1 = evaluate_model(\n",
    "    et_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Extra Trees\"\n",
    ")\n",
    "\n",
    "# ====================== MODEL 2: GRADIENT BOOSTING ======================\n",
    "print(\"\\n2. Training Gradient Boosting Classifier\")\n",
    "\n",
    "# Define parameter grid\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "gb_base = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "gb_cv = RandomizedSearchCV(\n",
    "    gb_base, gb_param_grid, n_iter=200, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "gb_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "gb_best = gb_cv.best_estimator_\n",
    "print(f\"Best parameters: {gb_cv.best_params_}\")\n",
    "\n",
    "# Evaluate\n",
    "gb_model, gb_accuracy, gb_f1 = evaluate_model(\n",
    "    gb_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Gradient Boosting\"\n",
    ")\n",
    "\n",
    "# ====================== MODEL 3: RANDOM FOREST ======================\n",
    "print(\"\\n3. Training Random Forest Classifier\")\n",
    "\n",
    "# Define parameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "rf_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "rf_cv = RandomizedSearchCV(\n",
    "    rf_base, rf_param_grid, n_iter=200, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "rf_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "rf_best = rf_cv.best_estimator_\n",
    "print(f\"Best parameters: {rf_cv.best_params_}\")\n",
    "\n",
    "# Evaluate\n",
    "rf_model, rf_accuracy, rf_f1 = evaluate_model(\n",
    "    rf_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Random Forest\"\n",
    ")\n",
    "\n",
    "# ====================== MODEL 4: XGBOOST ======================\n",
    "print(\"\\n4. Training XGBoost Classifier\")\n",
    "\n",
    "# Define parameter grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "xgb_base = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# RandomizedSearchCV\n",
    "xgb_cv = RandomizedSearchCV(\n",
    "    xgb_base, xgb_param_grid, n_iter=200, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "xgb_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "xgb_best = xgb_cv.best_estimator_\n",
    "print(f\"Best parameters: {xgb_cv.best_params_}\")\n",
    "\n",
    "# Evaluate\n",
    "xgb_model, xgb_accuracy, xgb_f1 = evaluate_model(\n",
    "    xgb_best, X_train_scaled, X_test_scaled, y_train, y_test, \"XGBoost\"\n",
    ")\n",
    "\n",
    "# ====================== MODEL 5: LOGISTIC REGRESSION ======================\n",
    "print(\"\\n5. Training Logistic Regression\")\n",
    "\n",
    "# Define parameter grid\n",
    "lr_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 7),\n",
    "    'penalty': ['l1', 'l2', None],\n",
    "    'solver': ['liblinear', 'saga', 'lbfgs'],\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "lr_base = LogisticRegression(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "lr_cv = RandomizedSearchCV(\n",
    "    lr_base, lr_param_grid, n_iter=50, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "lr_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "lr_best = lr_cv.best_estimator_\n",
    "print(f\"Best parameters: {lr_cv.best_params_}\")\n",
    "\n",
    "# Evaluate\n",
    "lr_model, lr_accuracy, lr_f1 = evaluate_model(\n",
    "    lr_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Logistic Regression\"\n",
    ")\n",
    "\n",
    "# ====================== ENSEMBLE MODEL: VOTING CLASSIFIER ======================\n",
    "print(\"\\n6. Training Voting Classifier Ensemble\")\n",
    "\n",
    "# Create a dictionary of our models\n",
    "models = {\n",
    "    'ExtraTrees': et_best,\n",
    "    'GradientBoosting': gb_best,\n",
    "    'RandomForest': rf_best,\n",
    "    'XGBoost': xgb_best,\n",
    "    'LogisticRegression': lr_best\n",
    "}\n",
    "\n",
    "# Calculate scores\n",
    "model_scores = {\n",
    "    'ExtraTrees': et_f1,\n",
    "    'GradientBoosting': gb_f1,\n",
    "    'RandomForest': rf_f1,\n",
    "    'XGBoost': xgb_f1,\n",
    "    'LogisticRegression': lr_f1\n",
    "}\n",
    "\n",
    "# Sort by F1 score and select top 3\n",
    "top_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "print(f\"Top 3 models for ensemble: {[model[0] for model in top_models]}\")\n",
    "\n",
    "# Create voting classifier with top 3 models\n",
    "estimators = [(name, models[name]) for name, _ in top_models]\n",
    "voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "# Evaluate\n",
    "voting_model, voting_accuracy, voting_f1 = evaluate_model(\n",
    "    voting_clf, X_train_scaled, X_test_scaled, y_train, y_test, \"Voting Classifier\"\n",
    ")\n",
    "\n",
    "# ====================== SUMMARY ======================\n",
    "print(\"\\n=== Model Performance Summary ===\")\n",
    "models_summary = {\n",
    "    'Extra Trees': (et_accuracy, et_f1),\n",
    "    'Gradient Boosting': (gb_accuracy, gb_f1),\n",
    "    'Random Forest': (rf_accuracy, rf_f1),\n",
    "    'XGBoost': (xgb_accuracy, xgb_f1),\n",
    "    'Logistic Regression': (lr_accuracy, lr_f1),\n",
    "    'Voting Classifier': (voting_accuracy, voting_f1)\n",
    "}\n",
    "\n",
    "# Sort by F1 score\n",
    "sorted_models = sorted(models_summary.items(), key=lambda x: x[1][1], reverse=True)\n",
    "\n",
    "print(\"\\nModels ranked by F1 score:\")\n",
    "for i, (model_name, (acc, f1)) in enumerate(sorted_models, 1):\n",
    "    print(f\"{i}. {model_name}: Accuracy = {acc:.4f}, F1 = {f1:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model_name = sorted_models[0][0]\n",
    "best_model = None\n",
    "\n",
    "if best_model_name == 'Extra Trees':\n",
    "    best_model = et_model\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_model = gb_model\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "elif best_model_name == 'XGBoost':\n",
    "    best_model = xgb_model\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    best_model = lr_model\n",
    "else:\n",
    "    best_model = voting_model\n",
    "\n",
    "import joblib\n",
    "joblib.dump(best_model, f'best_model_{best_model_name.replace(\" \", \"_\")}.pkl')\n",
    "print(f\"\\nBest model ({best_model_name}) saved as 'best_model_{best_model_name.replace(' ', '_')}.pkl'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3fe7486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing output files...\n",
      "Moved Extra_Trees_confusion_matrix.png to dfigures/\n",
      "Moved Extra_Trees_feature_importance.png to dfigures/\n",
      "Moved Extra_Trees_roc_curve.png to dfigures/\n",
      "Moved Gradient_Boosting_confusion_matrix.png to dfigures/\n",
      "Moved Gradient_Boosting_feature_importance.png to dfigures/\n",
      "Moved Gradient_Boosting_roc_curve.png to dfigures/\n",
      "Moved Logistic_Regression_confusion_matrix.png to dfigures/\n",
      "Moved Logistic_Regression_roc_curve.png to dfigures/\n",
      "Moved Random_Forest_confusion_matrix.png to dfigures/\n",
      "Moved Random_Forest_feature_importance.png to dfigures/\n",
      "Moved Random_Forest_roc_curve.png to dfigures/\n",
      "Moved Voting_Classifier_confusion_matrix.png to dfigures/\n",
      "Moved Voting_Classifier_roc_curve.png to dfigures/\n",
      "Moved XGBoost_confusion_matrix.png to dfigures/\n",
      "Moved XGBoost_feature_importance.png to dfigures/\n",
      "Moved XGBoost_roc_curve.png to dfigures/\n",
      "Moved best_model_Gradient_Boosting.pkl to models/\n",
      "Organization complete. 16 figures moved to 'dfigures' folder and 1 models moved to 'models' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def organize_output_files():\n",
    "    \"\"\"\n",
    "    Organize output files from the power grid attack detection model:\n",
    "    - Move all PNG files to a 'dfigures' folder\n",
    "    - Move all model files (.pkl) to a 'models' folder\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs('dfigures', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    print(\"Organizing output files...\")\n",
    "    \n",
    "    # Move PNG files to 'dfigures' folder\n",
    "    png_files = glob.glob('*.png')\n",
    "    for file in png_files:\n",
    "        dest_path = os.path.join('dfigures', file)\n",
    "        if os.path.exists(dest_path):\n",
    "            os.remove(dest_path)  # Remove if exists to avoid errors\n",
    "        shutil.move(file, 'dfigures')\n",
    "        print(f\"Moved {file} to dfigures/\")\n",
    "    \n",
    "    # Move model files to 'models' folder\n",
    "    model_files = glob.glob('*.pkl')\n",
    "    for file in model_files:\n",
    "        dest_path = os.path.join('models', file)\n",
    "        if os.path.exists(dest_path):\n",
    "            os.remove(dest_path)  # Remove if exists to avoid errors\n",
    "        shutil.move(file, 'models')\n",
    "        print(f\"Moved {file} to models/\")\n",
    "    \n",
    "    print(f\"Organization complete. {len(png_files)} figures moved to 'dfigures' folder and {len(model_files)} models moved to 'models' folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    organize_output_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d4d117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analyzing Missed Attacks ===\n",
      "\n",
      "Total missed attacks: 0\n",
      "\n",
      "Distribution of missed attack types:\n",
      "\n",
      "Distribution of missed generators:\n",
      "Series([], Name: count, dtype: Int64)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['original_index'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(missed_gens)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Create a detailed report of all missed attacks\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m missed_attacks_report \u001b[38;5;241m=\u001b[39m \u001b[43mmissed_attacks\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype_of_attack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgen_attacked\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     40\u001b[0m missed_attacks_report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattack_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m missed_attacks_report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype_of_attack\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(attack_type_names)\n\u001b[0;32m     41\u001b[0m missed_attacks_report \u001b[38;5;241m=\u001b[39m missed_attacks_report\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype_of_attack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['original_index'] not in index\""
     ]
    }
   ],
   "source": [
    "# Add this to the end of your existing notebook\n",
    "\n",
    "print(\"\\n=== Analyzing Missed Attacks ===\")\n",
    "\n",
    "# Get the best model predictions on the test set\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Create a DataFrame with test set results\n",
    "test_results = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': y_pred,\n",
    "    'correct': y_test == y_pred\n",
    "})\n",
    "\n",
    "# Get the original indices from the test set\n",
    "test_indices = X_test.index.tolist()\n",
    "test_results['original_index'] = test_indices\n",
    "\n",
    "# Add the test results back to the original DataFrame for analysis\n",
    "df_test_subset = df.loc[test_indices].copy()\n",
    "df_test_subset['predicted'] = y_pred\n",
    "\n",
    "# Find the missed attacks (false negatives)\n",
    "missed_attacks = df_test_subset[(df_test_subset['attack'] == 1) & \n",
    "                               (df_test_subset['predicted'] == 0)]\n",
    "\n",
    "# Display information about missed attacks\n",
    "print(f\"\\nTotal missed attacks: {len(missed_attacks)}\")\n",
    "print(\"\\nDistribution of missed attack types:\")\n",
    "missed_types = missed_attacks['type_of_attack'].value_counts()\n",
    "for attack_type, count in missed_types.items():\n",
    "    print(f\"  Attack Type {attack_type} ({attack_type_names.get(attack_type, 'Unknown')}): {count} instances\")\n",
    "\n",
    "print(\"\\nDistribution of missed generators:\")\n",
    "missed_gens = missed_attacks['gen_attacked'].value_counts().head(10)\n",
    "print(missed_gens)\n",
    "\n",
    "# Create a detailed report of all missed attacks\n",
    "missed_attacks_report = missed_attacks[['original_index', 'type_of_attack', 'gen_attacked']].copy()\n",
    "missed_attacks_report['attack_name'] = missed_attacks_report['type_of_attack'].map(attack_type_names)\n",
    "missed_attacks_report = missed_attacks_report.sort_values('type_of_attack')\n",
    "\n",
    "print(\"\\nDetailed report of missed attacks (first 20 rows):\")\n",
    "print(missed_attacks_report.head(20))\n",
    "\n",
    "# Save the full report to CSV\n",
    "missed_attacks_report.to_csv('missed_attacks_report.csv', index=False)\n",
    "print(f\"\\nFull report saved to 'missed_attacks_report.csv' with {len(missed_attacks_report)} rows\")\n",
    "\n",
    "# Optional: Analyze if missed attacks have patterns\n",
    "print(\"\\nAnalyzing patterns in missed attacks...\")\n",
    "\n",
    "# Compare feature distributions between correctly detected and missed attacks\n",
    "attack_data = df_test_subset[df_test_subset['attack'] == 1].copy()\n",
    "attack_data['detected'] = attack_data['predicted'] == 1\n",
    "\n",
    "# Analyze a few key features\n",
    "important_features = [\n",
    "    'fval_change', \n",
    "    'fval_change_rate',\n",
    "    'fval_diff_lag_1',\n",
    "    'ramp_constraint_active',\n",
    "    'fval_change_std_3d',\n",
    "    'upper_limit_proximity',\n",
    "    'lower_limit_proximity',\n",
    "    'cost_anomaly_score'\n",
    "]\n",
    "\n",
    "for feature in important_features:\n",
    "    if feature in attack_data.columns:\n",
    "        detected_mean = attack_data[attack_data['detected']].get(feature, pd.Series()).mean()\n",
    "        missed_mean = attack_data[~attack_data['detected']].get(feature, pd.Series()).mean()\n",
    "        print(f\"{feature}: Detected mean = {detected_mean:.4f}, Missed mean = {missed_mean:.4f}\")\n",
    "\n",
    "# Analyze if certain attack types are more likely to be missed\n",
    "detected_by_type = attack_data.groupby('type_of_attack')['detected'].mean()\n",
    "print(\"\\nDetection rate by attack type:\")\n",
    "for attack_type, detection_rate in detected_by_type.items():\n",
    "    print(f\"  {attack_type_names.get(attack_type, 'Unknown')}: {detection_rate:.2%} detected\")\n",
    "\n",
    "# Analyze if certain generators are more likely to have missed attacks\n",
    "detected_by_gen = attack_data.groupby('gen_attacked')['detected'].mean()\n",
    "print(\"\\nGenerators with lowest detection rates (top 5):\")\n",
    "print(detected_by_gen.sort_values().head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
