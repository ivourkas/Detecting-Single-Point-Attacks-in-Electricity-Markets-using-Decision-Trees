{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153bfadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 CPU cores for parallel processing\n",
      "\n",
      "=== Loading data ===\n",
      "Dataset loaded with shape: (1920, 34036)\n",
      "\n",
      "=== Performing advanced feature engineering ===\n",
      "\n",
      "=== Performing domain-specific feature engineering ===\n",
      "\n",
      "1. Creating specialized minimum boundary detection features...\n",
      "3. Creating features for large changes without threshold activation...\n",
      "4. Creating temporal pattern detection features...\n",
      "5. Creating composite anomaly detection features...\n",
      "6. Creating interaction features with new composite features...\n",
      "Created 15 new interaction features\n",
      "\n",
      "Advanced feature engineering complete. Added 97 new features.\n",
      "New DataFrame shape: (1920, 34134)\n",
      "Found 4 unique sessions: <IntegerArray>\n",
      "[1, 2, 3, 4]\n",
      "Length: 4, dtype: Int64\n",
      "\n",
      "================================================================================\n",
      "Training models for Session 1\n",
      "================================================================================\n",
      "Session 1 dataset shape: (480, 34134)\n",
      "Total records: 480\n",
      "Number of attacks: 240\n",
      "Number of normal operations: 240\n",
      "Attack percentage: 50.00%\n",
      "Training set shape: (432, 34131)\n",
      "Test set shape: (48, 34131)\n",
      "\n",
      "Applying variance threshold...\n",
      "Features after variance thresholding: 13281\n",
      "\n",
      "Selecting most informative features...\n",
      "Features after selection: 100\n",
      "\n",
      "Top 20 most informative features:\n",
      "ft1331: 0.076908\n",
      "ft4086: 0.072045\n",
      "ft4420: 0.063860\n",
      "ft1610: 0.063175\n",
      "ft1618: 0.060651\n",
      "ft3809: 0.059162\n",
      "ft5837: 0.057362\n",
      "ft5921: 0.055499\n",
      "ft257: 0.055445\n",
      "ft5937: 0.055143\n",
      "ft1538: 0.054220\n",
      "ft5060: 0.050733\n",
      "ft1382: 0.050485\n",
      "ft98: 0.048093\n",
      "ft1367: 0.047618\n",
      "ft4202: 0.042951\n",
      "ft5630: 0.042893\n",
      "ft3087: 0.042766\n",
      "ft215: 0.039908\n",
      "ft841: 0.037662\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "1. Training Extra Trees Classifier for Session 1\n",
      "Best parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Extra Trees Results:\n",
      "Accuracy: 0.8958\n",
      "F1 Score: 0.9057\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 3 24]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        21\n",
      "           1       0.92      0.89      0.91        27\n",
      "\n",
      "    accuracy                           0.90        48\n",
      "   macro avg       0.89      0.90      0.89        48\n",
      "weighted avg       0.90      0.90      0.90        48\n",
      "\n",
      "ROC AUC: 0.9771\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_cos_10d: 0.099353\n",
      "change_threshold_exceeded: 0.082679\n",
      "fval_cos_10d_interaction: 0.077269\n",
      "large_change_no_threshold_score: 0.062939\n",
      "change_std_ratio_no_threshold: 0.049831\n",
      "large_change_no_threshold_magnitude: 0.046608\n",
      "fval_sin_20d: 0.035538\n",
      "fval_change: 0.034301\n",
      "day_in_ssn_x_large_change_no_threshold_score: 0.033097\n",
      "min_boundary_approach_rate: 0.031254\n",
      "\n",
      "2. Training Gradient Boosting Classifier for Session 1\n",
      "Best parameters: {'subsample': 0.9, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change: 0.224305\n",
      "fval_diff_lag_1: 0.157517\n",
      "fval_diff_lag_1_x_min_boundary_approach_score: 0.126071\n",
      "fval_cos_10d_interaction: 0.102545\n",
      "fval_change_jerk: 0.062250\n",
      "fval_cos_10d: 0.058922\n",
      "fval_change_x_min_boundary_approach_score: 0.050069\n",
      "fval_diff_lag_1_x_weighted_anomaly_score: 0.032576\n",
      "fval_change_x_weighted_anomaly_score: 0.021812\n",
      "ft19275: 0.015903\n",
      "\n",
      "3. Training Random Forest Classifier for Session 1\n",
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.7917\n",
      "F1 Score: 0.8077\n",
      "Confusion Matrix:\n",
      "[[17  4]\n",
      " [ 6 21]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.81      0.77        21\n",
      "           1       0.84      0.78      0.81        27\n",
      "\n",
      "    accuracy                           0.79        48\n",
      "   macro avg       0.79      0.79      0.79        48\n",
      "weighted avg       0.80      0.79      0.79        48\n",
      "\n",
      "ROC AUC: 0.9277\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change_x_weighted_anomaly_score: 0.106658\n",
      "min_boundary_approach_rate: 0.101196\n",
      "fval_change: 0.087462\n",
      "fval_diff_lag_1_x_min_boundary_approach_score: 0.074382\n",
      "fval_change_x_min_boundary_approach_score: 0.068940\n",
      "fval_diff_lag_1_x_weighted_anomaly_score: 0.052475\n",
      "fval_diff_lag_1: 0.048582\n",
      "fval_change_jerk: 0.036643\n",
      "fval_cos_10d_interaction: 0.035589\n",
      "large_change_no_threshold_magnitude: 0.034841\n",
      "\n",
      "4. Training XGBoost Classifier for Session 1\n",
      "Best parameters: {'subsample': 0.9, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change: 0.359843\n",
      "fval_diff_lag_1: 0.221592\n",
      "fval_change_x_min_boundary_approach_score: 0.112099\n",
      "min_boundary_approach_rate: 0.053744\n",
      "fval_change_jerk: 0.044688\n",
      "fval_cos_10d: 0.032758\n",
      "ft13733: 0.025232\n",
      "fval_cos_10d_interaction: 0.023107\n",
      "ft8046: 0.014170\n",
      "ft28320: 0.012329\n",
      "\n",
      "5. Training Logistic Regression for Session 1\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 1000, 'C': 1.0}\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.8125\n",
      "F1 Score: 0.8364\n",
      "Confusion Matrix:\n",
      "[[16  5]\n",
      " [ 4 23]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78        21\n",
      "           1       0.82      0.85      0.84        27\n",
      "\n",
      "    accuracy                           0.81        48\n",
      "   macro avg       0.81      0.81      0.81        48\n",
      "weighted avg       0.81      0.81      0.81        48\n",
      "\n",
      "ROC AUC: 0.7972\n",
      "\n",
      "6. Training Voting Classifier Ensemble for Session 1\n",
      "Top 3 models for ensemble: ['GradientBoosting', 'XGBoost', 'ExtraTrees']\n",
      "\n",
      "Voting Classifier Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "=== Model Performance Summary for Session 1 ===\n",
      "\n",
      "Models ranked by F1 score:\n",
      "1. Gradient Boosting: Accuracy = 1.0000, F1 = 1.0000\n",
      "2. XGBoost: Accuracy = 1.0000, F1 = 1.0000\n",
      "3. Voting Classifier: Accuracy = 1.0000, F1 = 1.0000\n",
      "4. Extra Trees: Accuracy = 0.8958, F1 = 0.9057\n",
      "5. Logistic Regression: Accuracy = 0.8125, F1 = 0.8364\n",
      "6. Random Forest: Accuracy = 0.7917, F1 = 0.8077\n",
      "\n",
      "Best model for Session 1 (Gradient Boosting) saved as 'models/session_models/ssn1_gradient_boosting.pkl'\n",
      "\n",
      "Session 1 training completed in 756.41 seconds\n",
      "\n",
      "================================================================================\n",
      "Training models for Session 2\n",
      "================================================================================\n",
      "Session 2 dataset shape: (480, 34134)\n",
      "Total records: 480\n",
      "Number of attacks: 240\n",
      "Number of normal operations: 240\n",
      "Attack percentage: 50.00%\n",
      "Training set shape: (432, 34131)\n",
      "Test set shape: (48, 34131)\n",
      "\n",
      "Applying variance threshold...\n",
      "Features after variance thresholding: 13017\n",
      "\n",
      "Selecting most informative features...\n",
      "Features after selection: 100\n",
      "\n",
      "Top 20 most informative features:\n",
      "ft5803: 0.076822\n",
      "ft8061: 0.070771\n",
      "ft5837: 0.061713\n",
      "ft5212: 0.060693\n",
      "ft5621: 0.055154\n",
      "ft263: 0.053026\n",
      "ft9758: 0.052908\n",
      "ft5800: 0.051818\n",
      "ft2668: 0.047643\n",
      "ft7046: 0.046845\n",
      "ft122: 0.046564\n",
      "ft9391: 0.046386\n",
      "ft4385: 0.046241\n",
      "ft1683: 0.044000\n",
      "ft2374: 0.042554\n",
      "ft10185: 0.042315\n",
      "ft10186: 0.040883\n",
      "ft922: 0.039968\n",
      "ft250: 0.039348\n",
      "ft8700: 0.038710\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "1. Training Extra Trees Classifier for Session 2\n",
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Extra Trees Results:\n",
      "Accuracy: 0.9375\n",
      "F1 Score: 0.9412\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 3 24]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93        21\n",
      "           1       1.00      0.89      0.94        27\n",
      "\n",
      "    accuracy                           0.94        48\n",
      "   macro avg       0.94      0.94      0.94        48\n",
      "weighted avg       0.95      0.94      0.94        48\n",
      "\n",
      "ROC AUC: 0.9912\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_cos_10d: 0.108955\n",
      "fval_cos_10d_interaction: 0.088701\n",
      "change_threshold_exceeded: 0.068383\n",
      "large_change_no_threshold_score: 0.057449\n",
      "change_std_ratio_no_threshold: 0.053702\n",
      "large_change_no_threshold_magnitude: 0.038280\n",
      "fval_sin_20d: 0.037074\n",
      "day_in_ssn_x_large_change_no_threshold_score: 0.034536\n",
      "fval_sin_20d_interaction: 0.030936\n",
      "fval_cos_20d: 0.029488\n",
      "\n",
      "2. Training Gradient Boosting Classifier for Session 2\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "min_boundary_approach_rate: 0.610479\n",
      "fval_cos_10d: 0.084464\n",
      "fval_cos_10d_interaction: 0.068849\n",
      "fval_change_jerk: 0.049276\n",
      "ft8061: 0.025759\n",
      "ft11594: 0.018871\n",
      "change_std_ratio_no_threshold: 0.011879\n",
      "fval_diff_lag_1_x_min_boundary_approach_score: 0.009997\n",
      "ft25601: 0.009181\n",
      "day_in_ssn_x_weighted_anomaly_score: 0.008612\n",
      "\n",
      "3. Training Random Forest Classifier for Session 2\n",
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.8958\n",
      "F1 Score: 0.9057\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 3 24]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        21\n",
      "           1       0.92      0.89      0.91        27\n",
      "\n",
      "    accuracy                           0.90        48\n",
      "   macro avg       0.89      0.90      0.89        48\n",
      "weighted avg       0.90      0.90      0.90        48\n",
      "\n",
      "ROC AUC: 0.9753\n",
      "\n",
      "Top 10 most important features:\n",
      "min_boundary_approach_rate: 0.115647\n",
      "fval_diff_lag_1: 0.074311\n",
      "fval_change: 0.070508\n",
      "fval_change_x_weighted_anomaly_score: 0.067906\n",
      "fval_diff_lag_1_x_weighted_anomaly_score: 0.066648\n",
      "fval_change_x_min_boundary_approach_score: 0.064146\n",
      "fval_cos_10d_interaction: 0.034714\n",
      "fval_cos_10d: 0.034465\n",
      "large_change_no_threshold_score: 0.032924\n",
      "change_std_ratio_no_threshold: 0.030194\n",
      "\n",
      "4. Training XGBoost Classifier for Session 2\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "min_boundary_approach_rate: 0.353973\n",
      "fval_change: 0.192848\n",
      "fval_diff_lag_1: 0.140735\n",
      "fval_change_jerk: 0.053640\n",
      "fval_change_x_min_boundary_approach_score: 0.046589\n",
      "fval_cos_10d: 0.040738\n",
      "ft32001: 0.024190\n",
      "change_std_ratio_no_threshold: 0.020956\n",
      "fval_change_std_3d_x_large_change_no_threshold_score: 0.018399\n",
      "fval_cos_10d_interaction: 0.015761\n",
      "\n",
      "5. Training Logistic Regression for Session 2\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 1000, 'C': 1.0}\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.7917\n",
      "F1 Score: 0.8000\n",
      "Confusion Matrix:\n",
      "[[18  3]\n",
      " [ 7 20]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.86      0.78        21\n",
      "           1       0.87      0.74      0.80        27\n",
      "\n",
      "    accuracy                           0.79        48\n",
      "   macro avg       0.79      0.80      0.79        48\n",
      "weighted avg       0.80      0.79      0.79        48\n",
      "\n",
      "ROC AUC: 0.8783\n",
      "\n",
      "6. Training Voting Classifier Ensemble for Session 2\n",
      "Top 3 models for ensemble: ['GradientBoosting', 'XGBoost', 'ExtraTrees']\n",
      "\n",
      "Voting Classifier Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "=== Model Performance Summary for Session 2 ===\n",
      "\n",
      "Models ranked by F1 score:\n",
      "1. Gradient Boosting: Accuracy = 1.0000, F1 = 1.0000\n",
      "2. XGBoost: Accuracy = 1.0000, F1 = 1.0000\n",
      "3. Voting Classifier: Accuracy = 1.0000, F1 = 1.0000\n",
      "4. Extra Trees: Accuracy = 0.9375, F1 = 0.9412\n",
      "5. Random Forest: Accuracy = 0.8958, F1 = 0.9057\n",
      "6. Logistic Regression: Accuracy = 0.7917, F1 = 0.8000\n",
      "\n",
      "Best model for Session 2 (Gradient Boosting) saved as 'models/session_models/ssn2_gradient_boosting.pkl'\n",
      "\n",
      "Session 2 training completed in 481.46 seconds\n",
      "\n",
      "================================================================================\n",
      "Training models for Session 3\n",
      "================================================================================\n",
      "Session 3 dataset shape: (480, 34134)\n",
      "Total records: 480\n",
      "Number of attacks: 240\n",
      "Number of normal operations: 240\n",
      "Attack percentage: 50.00%\n",
      "Training set shape: (432, 34131)\n",
      "Test set shape: (48, 34131)\n",
      "\n",
      "Applying variance threshold...\n",
      "Features after variance thresholding: 13776\n",
      "\n",
      "Selecting most informative features...\n",
      "Features after selection: 100\n",
      "\n",
      "Top 20 most informative features:\n",
      "ft2224: 0.071459\n",
      "ft5634: 0.070166\n",
      "ft2340: 0.065282\n",
      "ft6643: 0.062209\n",
      "ft7038: 0.054809\n",
      "ft5167: 0.053969\n",
      "ft2632: 0.047104\n",
      "ft1215: 0.046692\n",
      "ft965: 0.045610\n",
      "ft122: 0.045282\n",
      "ft5060: 0.044539\n",
      "ft2301: 0.041201\n",
      "ft1610: 0.040648\n",
      "ft7310: 0.040646\n",
      "ft5794: 0.040086\n",
      "ft2800: 0.040032\n",
      "ft7039: 0.039670\n",
      "ft4210: 0.039085\n",
      "ft1678: 0.038323\n",
      "ft1667: 0.038227\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "1. Training Extra Trees Classifier for Session 3\n",
      "Best parameters: {'n_estimators': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Extra Trees Results:\n",
      "Accuracy: 0.9375\n",
      "F1 Score: 0.9434\n",
      "Confusion Matrix:\n",
      "[[20  1]\n",
      " [ 2 25]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93        21\n",
      "           1       0.96      0.93      0.94        27\n",
      "\n",
      "    accuracy                           0.94        48\n",
      "   macro avg       0.94      0.94      0.94        48\n",
      "weighted avg       0.94      0.94      0.94        48\n",
      "\n",
      "ROC AUC: 0.9903\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_cos_10d: 0.096991\n",
      "fval_cos_10d_interaction: 0.091908\n",
      "change_threshold_exceeded: 0.077796\n",
      "large_change_no_threshold_magnitude: 0.058025\n",
      "large_change_no_threshold_score: 0.052447\n",
      "fval_diff_lag_1: 0.041863\n",
      "fval_change: 0.040636\n",
      "day_in_ssn_x_large_change_no_threshold_score: 0.040037\n",
      "fval_sin_20d: 0.032292\n",
      "min_boundary_approach_rate: 0.028989\n",
      "\n",
      "2. Training Gradient Boosting Classifier for Session 3\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change: 0.277004\n",
      "min_boundary_approach_rate: 0.206593\n",
      "fval_cos_10d: 0.155877\n",
      "fval_cos_10d_interaction: 0.147395\n",
      "fval_diff_lag_1: 0.127900\n",
      "fval_ar3_residual: 0.019270\n",
      "ft10054: 0.009171\n",
      "ft2632: 0.008093\n",
      "fval_change_std_3d_x_large_change_no_threshold_score: 0.007782\n",
      "fval_change_std_3d_x_weighted_anomaly_score: 0.005390\n",
      "\n",
      "3. Training Random Forest Classifier for Session 3\n",
      "Best parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.8958\n",
      "F1 Score: 0.9057\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 3 24]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        21\n",
      "           1       0.92      0.89      0.91        27\n",
      "\n",
      "    accuracy                           0.90        48\n",
      "   macro avg       0.89      0.90      0.89        48\n",
      "weighted avg       0.90      0.90      0.90        48\n",
      "\n",
      "ROC AUC: 0.9788\n",
      "\n",
      "Top 10 most important features:\n",
      "min_boundary_approach_rate: 0.084520\n",
      "fval_change: 0.078135\n",
      "fval_diff_lag_1: 0.068352\n",
      "fval_diff_lag_1_x_weighted_anomaly_score: 0.068226\n",
      "fval_change_x_min_boundary_approach_score: 0.067959\n",
      "fval_change_x_weighted_anomaly_score: 0.060774\n",
      "fval_diff_lag_1_x_min_boundary_approach_score: 0.059820\n",
      "fval_cos_10d_interaction: 0.042311\n",
      "fval_cos_10d: 0.035998\n",
      "large_change_no_threshold_magnitude: 0.028819\n",
      "\n",
      "4. Training XGBoost Classifier for Session 3\n",
      "Best parameters: {'subsample': 0.9, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change_x_min_boundary_approach_score: 0.242626\n",
      "fval_diff_lag_1: 0.211408\n",
      "fval_change_x_weighted_anomaly_score: 0.189997\n",
      "min_boundary_approach_rate: 0.097781\n",
      "fval_change: 0.094534\n",
      "fval_ar3_residual: 0.035617\n",
      "fval_cos_10d: 0.024974\n",
      "weighted_anomaly_score: 0.019795\n",
      "ft28273: 0.014345\n",
      "fval_change_std_3d_x_large_change_no_threshold_score: 0.012463\n",
      "\n",
      "5. Training Logistic Regression for Session 3\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 1000, 'C': 10.0}\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.8750\n",
      "F1 Score: 0.8889\n",
      "Confusion Matrix:\n",
      "[[18  3]\n",
      " [ 3 24]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        21\n",
      "           1       0.89      0.89      0.89        27\n",
      "\n",
      "    accuracy                           0.88        48\n",
      "   macro avg       0.87      0.87      0.87        48\n",
      "weighted avg       0.88      0.88      0.88        48\n",
      "\n",
      "ROC AUC: 0.9012\n",
      "\n",
      "6. Training Voting Classifier Ensemble for Session 3\n",
      "Top 3 models for ensemble: ['GradientBoosting', 'XGBoost', 'ExtraTrees']\n",
      "\n",
      "Voting Classifier Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "=== Model Performance Summary for Session 3 ===\n",
      "\n",
      "Models ranked by F1 score:\n",
      "1. Gradient Boosting: Accuracy = 1.0000, F1 = 1.0000\n",
      "2. XGBoost: Accuracy = 1.0000, F1 = 1.0000\n",
      "3. Voting Classifier: Accuracy = 1.0000, F1 = 1.0000\n",
      "4. Extra Trees: Accuracy = 0.9375, F1 = 0.9434\n",
      "5. Random Forest: Accuracy = 0.8958, F1 = 0.9057\n",
      "6. Logistic Regression: Accuracy = 0.8750, F1 = 0.8889\n",
      "\n",
      "Best model for Session 3 (Gradient Boosting) saved as 'models/session_models/ssn3_gradient_boosting.pkl'\n",
      "\n",
      "Session 3 training completed in 492.93 seconds\n",
      "\n",
      "================================================================================\n",
      "Training models for Session 4\n",
      "================================================================================\n",
      "Session 4 dataset shape: (480, 34134)\n",
      "Total records: 480\n",
      "Number of attacks: 240\n",
      "Number of normal operations: 240\n",
      "Attack percentage: 50.00%\n",
      "Training set shape: (432, 34131)\n",
      "Test set shape: (48, 34131)\n",
      "\n",
      "Applying variance threshold...\n",
      "Features after variance thresholding: 13295\n",
      "\n",
      "Selecting most informative features...\n",
      "Features after selection: 100\n",
      "\n",
      "Top 20 most informative features:\n",
      "ft957: 0.065112\n",
      "ft841: 0.059595\n",
      "ft2387: 0.050316\n",
      "ft3749: 0.049493\n",
      "ft2377: 0.049202\n",
      "ft156: 0.048695\n",
      "ft2259: 0.048187\n",
      "ft1543: 0.046019\n",
      "ft5632: 0.044985\n",
      "ft3085: 0.043014\n",
      "ft265: 0.042911\n",
      "ft1540: 0.042090\n",
      "ft3719: 0.040554\n",
      "ft2792: 0.039695\n",
      "ft6645: 0.038905\n",
      "ft98: 0.038336\n",
      "ft7351: 0.038050\n",
      "ft3099: 0.037154\n",
      "ft5469: 0.037077\n",
      "ft969: 0.036408\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "1. Training Extra Trees Classifier for Session 4\n",
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Extra Trees Results:\n",
      "Accuracy: 0.9583\n",
      "F1 Score: 0.9615\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 2 25]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        21\n",
      "           1       1.00      0.93      0.96        27\n",
      "\n",
      "    accuracy                           0.96        48\n",
      "   macro avg       0.96      0.96      0.96        48\n",
      "weighted avg       0.96      0.96      0.96        48\n",
      "\n",
      "ROC AUC: 0.9982\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_cos_10d: 0.158110\n",
      "fval_cos_10d_interaction: 0.108883\n",
      "large_change_no_threshold_score: 0.060666\n",
      "large_change_no_threshold_magnitude: 0.047070\n",
      "fval_cos_20d: 0.045504\n",
      "fval_sin_20d: 0.040834\n",
      "fval_change: 0.038518\n",
      "fval_cos_20d_interaction: 0.036417\n",
      "fval_diff_lag_1: 0.034414\n",
      "min_boundary_approach_rate: 0.033932\n",
      "\n",
      "2. Training Gradient Boosting Classifier for Session 4\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change: 0.188430\n",
      "fval_change_x_weighted_anomaly_score: 0.136360\n",
      "fval_cos_10d_interaction: 0.125811\n",
      "fval_diff_lag_1: 0.107792\n",
      "fval_cos_10d: 0.096855\n",
      "min_boundary_approach_rate: 0.087813\n",
      "fval_diff_lag_1_x_min_boundary_approach_score: 0.070714\n",
      "fval_change_jerk: 0.035565\n",
      "fval_change_x_min_boundary_approach_score: 0.029682\n",
      "change_std_ratio_no_threshold: 0.018052\n",
      "\n",
      "3. Training Random Forest Classifier for Session 4\n",
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 10, 'bootstrap': False}\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.8542\n",
      "F1 Score: 0.8627\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 5 22]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84        21\n",
      "           1       0.92      0.81      0.86        27\n",
      "\n",
      "    accuracy                           0.85        48\n",
      "   macro avg       0.85      0.86      0.85        48\n",
      "weighted avg       0.86      0.85      0.85        48\n",
      "\n",
      "ROC AUC: 0.9577\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_diff_lag_1_x_min_boundary_approach_score: 0.083964\n",
      "fval_change_x_weighted_anomaly_score: 0.083822\n",
      "min_boundary_approach_rate: 0.078970\n",
      "fval_diff_lag_1: 0.071918\n",
      "fval_change: 0.070843\n",
      "fval_change_x_min_boundary_approach_score: 0.067606\n",
      "change_std_ratio_no_threshold: 0.053788\n",
      "fval_diff_lag_1_x_weighted_anomaly_score: 0.047541\n",
      "large_change_no_threshold_score: 0.038444\n",
      "fval_cos_10d: 0.037817\n",
      "\n",
      "4. Training XGBoost Classifier for Session 4\n",
      "Best parameters: {'subsample': 0.9, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_diff_lag_1: 0.243055\n",
      "fval_diff_lag_1_x_min_boundary_approach_score: 0.202162\n",
      "fval_change_x_min_boundary_approach_score: 0.139902\n",
      "fval_change: 0.135482\n",
      "min_boundary_approach_rate: 0.081592\n",
      "change_std_ratio_no_threshold: 0.036685\n",
      "fval_change_std_3d_x_large_change_no_threshold_score: 0.021642\n",
      "ft23661: 0.021219\n",
      "fval_cos_10d: 0.019269\n",
      "fval_change_jerk: 0.016515\n",
      "\n",
      "5. Training Logistic Regression for Session 4\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 1000, 'C': 10.0}\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.7917\n",
      "F1 Score: 0.8148\n",
      "Confusion Matrix:\n",
      "[[16  5]\n",
      " [ 5 22]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76        21\n",
      "           1       0.81      0.81      0.81        27\n",
      "\n",
      "    accuracy                           0.79        48\n",
      "   macro avg       0.79      0.79      0.79        48\n",
      "weighted avg       0.79      0.79      0.79        48\n",
      "\n",
      "ROC AUC: 0.8042\n",
      "\n",
      "6. Training Voting Classifier Ensemble for Session 4\n",
      "Top 3 models for ensemble: ['GradientBoosting', 'XGBoost', 'ExtraTrees']\n",
      "\n",
      "Voting Classifier Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "=== Model Performance Summary for Session 4 ===\n",
      "\n",
      "Models ranked by F1 score:\n",
      "1. Gradient Boosting: Accuracy = 1.0000, F1 = 1.0000\n",
      "2. XGBoost: Accuracy = 1.0000, F1 = 1.0000\n",
      "3. Voting Classifier: Accuracy = 1.0000, F1 = 1.0000\n",
      "4. Extra Trees: Accuracy = 0.9583, F1 = 0.9615\n",
      "5. Random Forest: Accuracy = 0.8542, F1 = 0.8627\n",
      "6. Logistic Regression: Accuracy = 0.7917, F1 = 0.8148\n",
      "\n",
      "Best model for Session 4 (Gradient Boosting) saved as 'models/session_models/ssn4_gradient_boosting.pkl'\n",
      "\n",
      "Session 4 training completed in 507.38 seconds\n",
      "\n",
      "=== Session Models Summary ===\n",
      "\n",
      "Model performance by session:\n",
      "+-------------------+-------------------+-------------------+-------------------+\n",
      "|Session            |Best Model         |Accuracy           |F1 Score           |\n",
      "+-------------------+-------------------+-------------------+-------------------+\n",
      "|1                  |Gradient Boosting  |1.0000             |1.0000             |\n",
      "|2                  |Gradient Boosting  |1.0000             |1.0000             |\n",
      "|3                  |Gradient Boosting  |1.0000             |1.0000             |\n",
      "|4                  |Gradient Boosting  |1.0000             |1.0000             |\n",
      "+-------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "Best performing session (Accuracy): Session 1 with 1.0000\n",
      "Worst performing session (Accuracy): Session 1 with 1.0000\n",
      "Best performing session (F1 Score): Session 1 with 1.0000\n",
      "Worst performing session (F1 Score): Session 1 with 1.0000\n",
      "\n",
      "=== Creating cross-session model comparison ===\n",
      "\n",
      "All session-specific models trained in 2346.13 seconds\n",
      "Results summary saved to 'session_figures/summaries/' directory\n",
      "Individual session figures saved to:\n",
      "  - 'session_figures/confusion_matrices/' for confusion matrices\n",
      "  - 'session_figures/roc_curves/' for ROC curves\n",
      "  - 'session_figures/feature_importance/' for feature importance plots\n",
      "All models saved to 'models/session_models/' directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the number of cores for parallel processing\n",
    "n_cores = max(1, os.cpu_count() // 2)\n",
    "print(f\"Using {n_cores} CPU cores for parallel processing\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('models/session_models', exist_ok=True)\n",
    "os.makedirs('session_figures', exist_ok=True)  # Separate folder for figures\n",
    "os.makedirs('session_figures/confusion_matrices', exist_ok=True)\n",
    "os.makedirs('session_figures/roc_curves', exist_ok=True)\n",
    "os.makedirs('session_figures/feature_importance', exist_ok=True)\n",
    "os.makedirs('session_figures/summaries', exist_ok=True)\n",
    "\n",
    "# Define a function to train models for a specific session\n",
    "def train_session_model(df, session_id, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Train all five models on data from a specific session with RandomizedSearchCV\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The full dataset\n",
    "    session_id : int\n",
    "        The session ID to filter by\n",
    "    test_size : float\n",
    "        The proportion of data to use for testing (default: 0.1 for 90-10 split)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training models for Session {session_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Filter data for this session\n",
    "    session_df = df[df['ssn'] == session_id].copy()\n",
    "    print(f\"Session {session_id} dataset shape: {session_df.shape}\")\n",
    "    \n",
    "    # Create binary target column if not already present\n",
    "    if 'attack' not in session_df.columns:\n",
    "        session_df['attack'] = (session_df['type_of_attack'] != 0).astype(int)  # 1 for attack, 0 for no attack\n",
    "    \n",
    "    # Display class distribution\n",
    "    attack_count = session_df['attack'].sum()\n",
    "    total_records = len(session_df)\n",
    "    normal_count = total_records - attack_count\n",
    "    print(f\"Total records: {total_records}\")\n",
    "    print(f\"Number of attacks: {attack_count}\")\n",
    "    print(f\"Number of normal operations: {normal_count}\")\n",
    "    print(f\"Attack percentage: {attack_count/total_records:.2%}\")\n",
    "    \n",
    "    # If either class has too few samples, notify but continue\n",
    "    if attack_count < 5 or normal_count < 5:\n",
    "        print(f\"WARNING: Session {session_id} has very few samples in one class.\")\n",
    "    \n",
    "    # Define feature columns, excluding the target and direct identifiers\n",
    "    feature_columns = [col for col in session_df.columns if col not in ['attack', 'type_of_attack', 'gen_attacked']]\n",
    "    \n",
    "    X = session_df[feature_columns]\n",
    "    y = session_df['attack']\n",
    "    \n",
    "    # Split dataset into training and testing sets (90-10 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=None)\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Feature selection and preprocessing pipeline\n",
    "    # 1. Remove low variance features\n",
    "    print(\"\\nApplying variance threshold...\")\n",
    "    variance_threshold = 0.01\n",
    "    selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    X_train_var = selector.fit_transform(X_train)\n",
    "    X_test_var = selector.transform(X_test)\n",
    "    print(f\"Features after variance thresholding: {X_train_var.shape[1]}\")\n",
    "    \n",
    "    # 2. Feature selection using mutual information\n",
    "    print(\"\\nSelecting most informative features...\")\n",
    "    select_k = SelectKBest(mutual_info_classif, k=min(100, X_train_var.shape[1]))\n",
    "    X_train_selected = select_k.fit_transform(X_train_var, y_train)\n",
    "    X_test_selected = select_k.transform(X_test_var)\n",
    "    print(f\"Features after selection: {X_train_selected.shape[1]}\")\n",
    "    \n",
    "    # Print top 20 feature names\n",
    "    selected_indices = select_k.get_support(indices=True)\n",
    "    original_indices = selector.get_support(indices=True)\n",
    "    selected_names = [X.columns[original_indices[i]] for i in selected_indices[:20]]\n",
    "    scores = select_k.scores_[selected_indices]\n",
    "    \n",
    "    print(\"\\nTop 20 most informative features:\")\n",
    "    for name, score in sorted(zip(selected_names, scores), key=lambda x: x[1], reverse=True)[:20]:\n",
    "        print(f\"{name}: {score:.6f}\")\n",
    "    \n",
    "    # 3. Scale features\n",
    "    print(\"\\nScaling features...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "    \n",
    "    # Define a function to evaluate models\n",
    "    def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "        # Create session-specific filename prefix\n",
    "        prefix = f\"ssn{session_id}_{model_name.replace(' ', '_').lower()}\"\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['Normal', 'Attack'],\n",
    "                    yticklabels=['Normal', 'Attack'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(f'Session {session_id} - Confusion Matrix - {model_name}')\n",
    "        plt.savefig(f'session_figures/confusion_matrices/{prefix}_confusion_matrix.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # ROC curve and AUC (if applicable)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            try:\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "                \n",
    "                # Plot ROC curve\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                        label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'Session {session_id} - ROC Curve - {model_name}')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.savefig(f'session_figures/roc_curves/{prefix}_roc_curve.png')\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not calculate ROC AUC: {e}\")\n",
    "                \n",
    "        # Feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_names = [X.columns[original_indices[i]] for i in selected_indices]\n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            # Plot top 20 feature importances\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.title(f'Session {session_id} - Top 20 Feature Importances - {model_name}')\n",
    "            n_features = min(20, len(importances))\n",
    "            plt.bar(range(n_features), \n",
    "                    importances[indices[:n_features]], align='center')\n",
    "            plt.xticks(range(n_features), \n",
    "                    [feature_names[i] for i in indices[:n_features]], rotation=90)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'session_figures/feature_importance/{prefix}_feature_importance.png')\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"\\nTop 10 most important features:\")\n",
    "            for i in range(min(10, len(importances))):\n",
    "                print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.6f}\")\n",
    "        \n",
    "        return model, accuracy, f1\n",
    "\n",
    "    # ====================== MODEL 1: EXTRA TREES CLASSIFIER ======================\n",
    "    print(\"\\n1. Training Extra Trees Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    et_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    et_base = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    et_cv = RandomizedSearchCV(\n",
    "        et_base, et_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    et_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    et_best = et_cv.best_estimator_\n",
    "    print(f\"Best parameters: {et_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    et_model, et_accuracy, et_f1 = evaluate_model(\n",
    "        et_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Extra Trees\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 2: GRADIENT BOOSTING ======================\n",
    "    print(\"\\n2. Training Gradient Boosting Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    gb_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    gb_base = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    gb_cv = RandomizedSearchCV(\n",
    "        gb_base, gb_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    gb_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    gb_best = gb_cv.best_estimator_\n",
    "    print(f\"Best parameters: {gb_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    gb_model, gb_accuracy, gb_f1 = evaluate_model(\n",
    "        gb_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Gradient Boosting\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 3: RANDOM FOREST ======================\n",
    "    print(\"\\n3. Training Random Forest Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    rf_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    rf_cv = RandomizedSearchCV(\n",
    "        rf_base, rf_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    rf_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    rf_best = rf_cv.best_estimator_\n",
    "    print(f\"Best parameters: {rf_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    rf_model, rf_accuracy, rf_f1 = evaluate_model(\n",
    "        rf_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Random Forest\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 4: XGBOOST ======================\n",
    "    print(\"\\n4. Training XGBoost Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3],\n",
    "        'gamma': [0, 0.1],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    xgb_base = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    xgb_cv = RandomizedSearchCV(\n",
    "        xgb_base, xgb_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    xgb_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    xgb_best = xgb_cv.best_estimator_\n",
    "    print(f\"Best parameters: {xgb_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    xgb_model, xgb_accuracy, xgb_f1 = evaluate_model(\n",
    "        xgb_best, X_train_scaled, X_test_scaled, y_train, y_test, \"XGBoost\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 5: LOGISTIC REGRESSION ======================\n",
    "    print(\"\\n5. Training Logistic Regression for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    lr_param_grid = {\n",
    "        'C': np.logspace(-3, 3, 7),\n",
    "        'penalty': ['l1', 'l2', None],\n",
    "        'solver': ['liblinear', 'saga', 'lbfgs'],\n",
    "        'max_iter': [1000, 2000]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    lr_base = LogisticRegression(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    lr_cv = RandomizedSearchCV(\n",
    "        lr_base, lr_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    lr_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    lr_best = lr_cv.best_estimator_\n",
    "    print(f\"Best parameters: {lr_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    lr_model, lr_accuracy, lr_f1 = evaluate_model(\n",
    "        lr_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Logistic Regression\"\n",
    "    )\n",
    "\n",
    "    # ====================== ENSEMBLE MODEL: VOTING CLASSIFIER ======================\n",
    "    print(\"\\n6. Training Voting Classifier Ensemble for Session\", session_id)\n",
    "\n",
    "    # Create a dictionary of our models\n",
    "    models = {\n",
    "        'ExtraTrees': et_best,\n",
    "        'GradientBoosting': gb_best,\n",
    "        'RandomForest': rf_best,\n",
    "        'XGBoost': xgb_best,\n",
    "        'LogisticRegression': lr_best\n",
    "    }\n",
    "\n",
    "    # Calculate scores\n",
    "    model_scores = {\n",
    "        'ExtraTrees': et_f1,\n",
    "        'GradientBoosting': gb_f1,\n",
    "        'RandomForest': rf_f1,\n",
    "        'XGBoost': xgb_f1,\n",
    "        'LogisticRegression': lr_f1\n",
    "    }\n",
    "\n",
    "    # Sort by F1 score and select top 3\n",
    "    top_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(f\"Top 3 models for ensemble: {[model[0] for model in top_models]}\")\n",
    "\n",
    "    # Create voting classifier with top 3 models\n",
    "    estimators = [(name, models[name]) for name, _ in top_models]\n",
    "    voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "    # Evaluate\n",
    "    voting_model, voting_accuracy, voting_f1 = evaluate_model(\n",
    "        voting_clf, X_train_scaled, X_test_scaled, y_train, y_test, \"Voting Classifier\"\n",
    "    )\n",
    "\n",
    "    # ====================== SUMMARY ======================\n",
    "    print(\"\\n=== Model Performance Summary for Session\", session_id, \"===\")\n",
    "    models_summary = {\n",
    "        'Extra Trees': (et_accuracy, et_f1),\n",
    "        'Gradient Boosting': (gb_accuracy, gb_f1),\n",
    "        'Random Forest': (rf_accuracy, rf_f1),\n",
    "        'XGBoost': (xgb_accuracy, xgb_f1),\n",
    "        'Logistic Regression': (lr_accuracy, lr_f1),\n",
    "        'Voting Classifier': (voting_accuracy, voting_f1)\n",
    "    }\n",
    "\n",
    "    # Sort by F1 score\n",
    "    sorted_models = sorted(models_summary.items(), key=lambda x: x[1][1], reverse=True)\n",
    "\n",
    "    print(\"\\nModels ranked by F1 score:\")\n",
    "    for i, (model_name, (acc, f1)) in enumerate(sorted_models, 1):\n",
    "        print(f\"{i}. {model_name}: Accuracy = {acc:.4f}, F1 = {f1:.4f}\")\n",
    "\n",
    "    # Plot model comparison (both Accuracy and F1 Score)\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    model_names = [name for name, _ in sorted_models]\n",
    "    accuracies = [acc for _, (acc, _) in sorted_models]\n",
    "    f1_scores = [f1 for _, (_, f1) in sorted_models]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='skyblue')\n",
    "    rects2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='salmon')\n",
    "    \n",
    "    ax.set_xlabel('Models', fontsize=14)\n",
    "    ax.set_ylabel('Scores', fontsize=14)\n",
    "    ax.set_title(f'Session {session_id} - Model Performance Comparison', fontsize=16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=12)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_ylim(0, 1.1)  # Ensure consistent y-axis scale\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for rect in rects1:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    for rect in rects2:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Add a grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'session_figures/summaries/ssn{session_id}_model_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save the best model\n",
    "    best_model_name = sorted_models[0][0]\n",
    "    best_model = None\n",
    "\n",
    "    if best_model_name == 'Extra Trees':\n",
    "        best_model = et_model\n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        best_model = gb_model\n",
    "    elif best_model_name == 'Random Forest':\n",
    "        best_model = rf_model\n",
    "    elif best_model_name == 'XGBoost':\n",
    "        best_model = xgb_model\n",
    "    elif best_model_name == 'Logistic Regression':\n",
    "        best_model = lr_model\n",
    "    else:\n",
    "        best_model = voting_model\n",
    "\n",
    "    # Save model package\n",
    "    model_package = {\n",
    "        'variance_selector': selector,\n",
    "        'feature_selector': select_k,\n",
    "        'scaler': scaler,\n",
    "        'model': best_model,\n",
    "        'feature_columns': feature_columns,\n",
    "        'selected_indices': selected_indices,\n",
    "        'original_indices': original_indices,\n",
    "        'best_model_name': best_model_name,\n",
    "        'metrics': {'accuracy': sorted_models[0][1][0], 'f1': sorted_models[0][1][1]},\n",
    "        'all_models': {\n",
    "            'ExtraTrees': et_model,\n",
    "            'GradientBoosting': gb_model,\n",
    "            'RandomForest': rf_model,\n",
    "            'XGBoost': xgb_model,\n",
    "            'LogisticRegression': lr_model,\n",
    "            'VotingClassifier': voting_model\n",
    "        },\n",
    "        'all_metrics': models_summary\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/session_models/ssn{session_id}_{best_model_name.replace(' ', '_').lower()}.pkl\"\n",
    "    joblib.dump(model_package, model_path)\n",
    "    print(f\"\\nBest model for Session {session_id} ({best_model_name}) saved as '{model_path}'\")\n",
    "    \n",
    "    print(f\"\\nSession {session_id} training completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return model_package\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n=== Loading data ===\")\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(\"data/N300_G69_transposed.csv\", dtype={'ssn':'Int64', 'type_of_attack': 'Int64', 'gen_attacked': 'Int64'})\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    if 'attack' not in df.columns:\n",
    "        df['attack'] = (df['type_of_attack'] != 0).astype(int)  # 1 for attack, 0 for no attack\n",
    "    \n",
    "    \n",
    "    # ====================== ADVANCED FEATURE ENGINEERING ======================\n",
    "    # Apply the advanced feature engineering \n",
    "    from advanced_features import engineer_advanced_features\n",
    "    df = engineer_advanced_features(df)\n",
    "        \n",
    "    # Get unique session IDs\n",
    "    session_ids = df['ssn'].unique()\n",
    "    print(f\"Found {len(session_ids)} unique sessions: {session_ids}\")\n",
    "    \n",
    "    # Train models for each session\n",
    "    session_results = {}\n",
    "    \n",
    "    for session_id in session_ids:\n",
    "        model_package = train_session_model(df, session_id)\n",
    "        session_results[session_id] = model_package['metrics']['f1']\n",
    "    \n",
    "    # Create a summary of results across sessions\n",
    "    print(\"\\n=== Session Models Summary ===\")\n",
    "    print(\"\\nModel performance by session:\")\n",
    "    \n",
    "    # Table format for terminal display\n",
    "    headers = [\"Session\", \"Best Model\", \"Accuracy\", \"F1 Score\"]\n",
    "    rows = []\n",
    "    \n",
    "    # Dictionary to store accuracy and F1 scores for each session\n",
    "    accuracy_by_session = {}\n",
    "    f1_by_session = {}\n",
    "    best_models_by_session = {}\n",
    "   # Fixed version - only show the best model per session\n",
    "    for session_id in session_ids:\n",
    "        best_model_file = None\n",
    "        # Find the file that corresponds to the best model for this session\n",
    "        for file in os.listdir(\"models/session_models/\"):\n",
    "            if file.startswith(f\"ssn{session_id}_\") and not file.endswith(\"_noscale.pkl\"):\n",
    "                # If we already found a file for this session, we need to determine which is newer/correct\n",
    "                if best_model_file is None:\n",
    "                    best_model_file = file\n",
    "                \n",
    "        if best_model_file:\n",
    "            model_path = os.path.join(\"models/session_models/\", best_model_file)\n",
    "            model_package = joblib.load(model_path)\n",
    "            \n",
    "            accuracy = model_package['metrics']['accuracy']\n",
    "            f1 = model_package['metrics']['f1']\n",
    "            best_model = model_package['best_model_name']\n",
    "            \n",
    "            accuracy_by_session[session_id] = accuracy\n",
    "            f1_by_session[session_id] = f1\n",
    "            best_models_by_session[session_id] = best_model\n",
    "            \n",
    "            # Only add one row per session\n",
    "            rows.append([str(session_id), str(best_model), f\"{accuracy:.4f}\", f\"{f1:.4f}\"])\n",
    "    \n",
    "    # Print table\n",
    "    col_width = max(len(word) for row in [headers] + rows for word in row) + 2\n",
    "    print(\"+\" + \"+\".join([\"-\" * col_width for _ in headers]) + \"+\")\n",
    "    print(\"|\" + \"|\".join(word.ljust(col_width) for word in headers) + \"|\")\n",
    "    print(\"+\" + \"+\".join([\"-\" * col_width for _ in headers]) + \"+\")\n",
    "    for row in rows:\n",
    "        print(\"|\" + \"|\".join(str(word).ljust(col_width) for word in row) + \"|\")\n",
    "    print(\"+\" + \"+\".join([\"-\" * col_width for _ in headers]) + \"+\")\n",
    "    \n",
    "    # Identify best and worst performing sessions by both metrics\n",
    "    best_acc_session = max(accuracy_by_session.items(), key=lambda x: x[1])\n",
    "    worst_acc_session = min(accuracy_by_session.items(), key=lambda x: x[1])\n",
    "    best_f1_session = max(f1_by_session.items(), key=lambda x: x[1])\n",
    "    worst_f1_session = min(f1_by_session.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\nBest performing session (Accuracy): Session {best_acc_session[0]} with {best_acc_session[1]:.4f}\")\n",
    "    print(f\"Worst performing session (Accuracy): Session {worst_acc_session[0]} with {worst_acc_session[1]:.4f}\")\n",
    "    print(f\"Best performing session (F1 Score): Session {best_f1_session[0]} with {best_f1_session[1]:.4f}\")\n",
    "    print(f\"Worst performing session (F1 Score): Session {worst_f1_session[0]} with {worst_f1_session[1]:.4f}\")\n",
    "    \n",
    "    # Plot summary of scores by session\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    sessions = list(sorted(accuracy_by_session.keys()))\n",
    "    accuracies = [accuracy_by_session[s] for s in sessions]\n",
    "    f1_scores = [f1_by_session[s] for s in sessions]\n",
    "    \n",
    "    x = np.arange(len(sessions))\n",
    "    width = 0.35\n",
    "    \n",
    "    rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='skyblue')\n",
    "    rects2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='salmon')\n",
    "    \n",
    "    ax.set_xlabel('Session ID', fontsize=14)\n",
    "    ax.set_ylabel('Score', fontsize=14)\n",
    "    ax.set_title('Attack Detection Performance by Session', fontsize=16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'Session {s}' for s in sessions], fontsize=12)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for rect in rects1:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "                    \n",
    "    for rect in rects2:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/session_performance_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot best model by session\n",
    "    model_counts = {}\n",
    "    for session_id in session_ids:\n",
    "        files = os.listdir(\"models/session_models/\")\n",
    "        for file in files:\n",
    "            if file.startswith(f\"ssn{session_id}_\"):\n",
    "                model_name = file.replace(f\"ssn{session_id}_\", \"\").replace(\".pkl\", \"\").replace(\"_\", \" \").title()\n",
    "                if model_name not in model_counts:\n",
    "                    model_counts[model_name] = 0\n",
    "                model_counts[model_name] += 1\n",
    "    \n",
    "    # Create a bar chart of best model types\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    models = list(model_counts.keys())\n",
    "    counts = [model_counts[m] for m in models]\n",
    "    \n",
    "    plt.bar(models, counts, color='lightgreen')\n",
    "    plt.xlabel('Model Type', fontsize=14)\n",
    "    plt.ylabel('Number of Sessions', fontsize=14)\n",
    "    plt.title('Best Model Type by Number of Sessions', fontsize=16)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for i, v in enumerate(counts):\n",
    "        plt.text(i, v + 0.1, str(v), ha='center', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/best_model_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create comparison table for all models across all sessions\n",
    "    print(\"\\n=== Creating cross-session model comparison ===\")\n",
    "    \n",
    "    # Dictionary to store F1 scores for each model type across sessions\n",
    "    model_performance = {\n",
    "        'Extra Trees': {},\n",
    "        'Gradient Boosting': {},\n",
    "        'Random Forest': {},\n",
    "        'XGBoost': {},\n",
    "        'Logistic Regression': {},\n",
    "        'Voting Classifier': {}\n",
    "    }\n",
    "    \n",
    "    # Load model packages to get performance metrics\n",
    "    for session_id in session_ids:\n",
    "        for file in os.listdir(\"models/session_models/\"):\n",
    "            if file.startswith(f\"ssn{session_id}_\"):\n",
    "                model_path = os.path.join(\"models/session_models/\", file)\n",
    "                model_package = joblib.load(model_path)\n",
    "                \n",
    "                # Extract all model metrics\n",
    "                for model_name, (acc, f1) in model_package['all_metrics'].items():\n",
    "                    model_performance[model_name][session_id] = f1\n",
    "    \n",
    "    # Create a heatmap of model performance across sessions\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Initialize data structures for both metrics\n",
    "    accuracy_heatmap_data = []\n",
    "    f1_heatmap_data = []\n",
    "    model_names = list(model_performance.keys())\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        # Collect accuracy scores\n",
    "        accuracy_scores = []\n",
    "        f1_scores = []\n",
    "        for session_id in session_ids:\n",
    "            # For each session, load the model package and get metrics for all models\n",
    "            found = False\n",
    "            for file in os.listdir(\"models/session_models/\"):\n",
    "                if file.startswith(f\"ssn{session_id}_\"):\n",
    "                    model_path = os.path.join(\"models/session_models/\", file)\n",
    "                    model_package = joblib.load(model_path)\n",
    "                    \n",
    "                    # Extract metrics for this model\n",
    "                    if model_name in model_package['all_metrics']:\n",
    "                        acc, f1 = model_package['all_metrics'][model_name]\n",
    "                        accuracy_scores.append(acc)\n",
    "                        f1_scores.append(f1)\n",
    "                        found = True\n",
    "                        break\n",
    "            \n",
    "            if not found:\n",
    "                accuracy_scores.append(0)\n",
    "                f1_scores.append(0)\n",
    "                \n",
    "        accuracy_heatmap_data.append(accuracy_scores)\n",
    "        f1_heatmap_data.append(f1_scores)\n",
    "    \n",
    "    # Create accuracy heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(accuracy_heatmap_data, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", \n",
    "                xticklabels=[f\"Session {s}\" for s in session_ids],\n",
    "                yticklabels=model_names,\n",
    "                vmin=0, vmax=1)\n",
    "    \n",
    "    plt.title('Accuracy by Model and Session', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/model_session_accuracy_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create F1 score heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(f1_heatmap_data, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", \n",
    "                xticklabels=[f\"Session {s}\" for s in session_ids],\n",
    "                yticklabels=model_names,\n",
    "                vmin=0, vmax=1)\n",
    "    \n",
    "    plt.title('F1 Score by Model and Session', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/model_session_f1_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create attack type distribution visualization\n",
    "    if 'type_of_attack' in df.columns:\n",
    "        attack_types = df[df['attack'] == 1]['type_of_attack'].value_counts()\n",
    "        \n",
    "        # Map attack types to descriptive names\n",
    "        attack_type_names = {\n",
    "            0: \"Normal\",\n",
    "            1: \"Ramp Rate Attack\",\n",
    "            2: \"Upper Limit Attack\",\n",
    "            3: \"Lower Limit Attack\",\n",
    "            4: \"Generation Cost Attack\"\n",
    "        }\n",
    "        \n",
    "        # Create a separate dataset for attack type distribution by session\n",
    "        attack_by_session = {}\n",
    "        for session_id in session_ids:\n",
    "            session_attacks = df[(df['ssn'] == session_id) & (df['attack'] == 1)]['type_of_attack'].value_counts()\n",
    "            attack_by_session[session_id] = {attack_type_names.get(k, f\"Type {k}\"): v \n",
    "                                           for k, v in session_attacks.items()}\n",
    "        \n",
    "        # Prepare data for stacked bar chart\n",
    "        attack_types_data = []\n",
    "        for attack_type in range(1, 5):  # Attack types 1-4\n",
    "            type_data = []\n",
    "            for session_id in session_ids:\n",
    "                if attack_type_names.get(attack_type) in attack_by_session[session_id]:\n",
    "                    type_data.append(attack_by_session[session_id][attack_type_names.get(attack_type)])\n",
    "                else:\n",
    "                    type_data.append(0)\n",
    "            attack_types_data.append(type_data)\n",
    "        \n",
    "        # Create stacked bar chart\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        bottom = np.zeros(len(session_ids))\n",
    "        \n",
    "        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "        \n",
    "        for i, attack_data in enumerate(attack_types_data):\n",
    "            plt.bar([f\"Session {s}\" for s in session_ids], attack_data, bottom=bottom, \n",
    "                   label=attack_type_names.get(i+1), color=colors[i])\n",
    "            bottom += attack_data\n",
    "        \n",
    "        plt.xlabel('Session', fontsize=14)\n",
    "        plt.ylabel('Number of Attacks', fontsize=14)\n",
    "        plt.title('Attack Type Distribution by Session', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.xticks(rotation=0, fontsize=12)\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('session_figures/summaries/attack_type_distribution_by_session.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Create pie charts for each session's attack distribution\n",
    "        for session_id in session_ids:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            session_data = df[(df['ssn'] == session_id) & (df['attack'] == 1)]\n",
    "            \n",
    "            if len(session_data) > 0:\n",
    "                attack_counts = session_data['type_of_attack'].value_counts()\n",
    "                labels = [attack_type_names.get(i, f\"Type {i}\") for i in attack_counts.index]\n",
    "                \n",
    "                plt.pie(attack_counts, labels=labels, autopct='%1.1f%%', \n",
    "                       startangle=90, colors=colors, wedgeprops={'edgecolor': 'black'})\n",
    "                plt.axis('equal')\n",
    "                plt.title(f'Session {session_id} - Attack Type Distribution', fontsize=16)\n",
    "                plt.savefig(f'session_figures/summaries/session{session_id}_attack_distribution_pie.png')\n",
    "                plt.close()\n",
    "    \n",
    "    print(f\"\\nAll session-specific models trained in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Results summary saved to 'session_figures/summaries/' directory\")\n",
    "    print(f\"Individual session figures saved to:\")\n",
    "    print(f\"  - 'session_figures/confusion_matrices/' for confusion matrices\")\n",
    "    print(f\"  - 'session_figures/roc_curves/' for ROC curves\")\n",
    "    print(f\"  - 'session_figures/feature_importance/' for feature importance plots\")\n",
    "    print(f\"All models saved to 'models/session_models/' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
