{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d68d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 CPU cores for parallel processing\n",
      "\n",
      "=== Loading and preprocessing data ===\n",
      "Dataset loaded with shape: (1920, 34036)\n",
      "Total records: 1920\n",
      "Number of attacks: 960\n",
      "Number of normal operations: 960\n",
      "Attack percentage: 50.00%\n",
      "\n",
      "Attack type distribution:\n",
      "type_of_attack\n",
      "3    276\n",
      "1    240\n",
      "2    236\n",
      "4    208\n",
      "Name: count, dtype: Int64\n",
      "Lower Limit Attack: 276 instances\n",
      "Ramp Rate Attack: 240 instances\n",
      "Upper Limit Attack: 236 instances\n",
      "Generation Cost Attack: 208 instances\n",
      "\n",
      "Top 10 attacked generators:\n",
      "gen_attacked\n",
      "25    40\n",
      "60    36\n",
      "19    28\n",
      "39    28\n",
      "52    28\n",
      "12    28\n",
      "29    28\n",
      "61    28\n",
      "23    28\n",
      "16    24\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "=== Performing advanced feature engineering ===\n",
      "\n",
      "=== Performing domain-specific feature engineering ===\n",
      "\n",
      "1. Creating specialized minimum boundary detection features...\n",
      "3. Creating features for large changes without threshold activation...\n",
      "4. Creating temporal pattern detection features...\n",
      "5. Creating composite anomaly detection features...\n",
      "6. Creating interaction features with new composite features...\n",
      "Created 15 new interaction features\n",
      "\n",
      "Advanced feature engineering complete. Added 97 new features.\n",
      "New DataFrame shape: (1920, 34134)\n",
      "\n",
      "=== Preparing features for modeling ===\n",
      "Training set shape: (1536, 34131)\n",
      "Test set shape: (384, 34131)\n",
      "\n",
      "Applying variance threshold...\n",
      "Features after variance thresholding: 18395\n",
      "\n",
      "Selecting most informative features...\n",
      "Features after selection: 100\n",
      "\n",
      "Top 20 most informative features:\n",
      "ft7003: 0.037120\n",
      "ft8630: 0.031534\n",
      "ft7167: 0.030494\n",
      "ft1681: 0.030205\n",
      "ft3087: 0.029362\n",
      "ft1215: 0.028145\n",
      "ft2934: 0.027412\n",
      "ft8382: 0.027335\n",
      "ft6555: 0.027107\n",
      "ft192: 0.026859\n",
      "ft5892: 0.026147\n",
      "ft1369: 0.025817\n",
      "ft2792: 0.025804\n",
      "ft7282: 0.025616\n",
      "ft5216: 0.025375\n",
      "ft805: 0.024980\n",
      "ft5937: 0.024643\n",
      "ft5137: 0.023784\n",
      "ft7221: 0.023260\n",
      "ft2800: 0.022800\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "=== Training and evaluating models ===\n",
      "\n",
      "1. Training Extra Trees Classifier\n",
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Extra Trees Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[190   0]\n",
      " [  0 194]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       190\n",
      "           1       1.00      1.00      1.00       194\n",
      "\n",
      "    accuracy                           1.00       384\n",
      "   macro avg       1.00      1.00      1.00       384\n",
      "weighted avg       1.00      1.00      1.00       384\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_cos_10d: 0.180547\n",
      "change_threshold_exceeded: 0.074578\n",
      "fval_cos_10d_interaction: 0.066538\n",
      "large_change_no_threshold_score: 0.059509\n",
      "fval_sin_20d: 0.054841\n",
      "fval_cos_20d: 0.041484\n",
      "large_change_no_threshold_magnitude: 0.038999\n",
      "day_in_ssn_x_large_change_no_threshold_score: 0.033375\n",
      "fval_change: 0.032219\n",
      "fval_sin_20d_interaction: 0.028290\n",
      "\n",
      "2. Training Gradient Boosting Classifier\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_curve, auc\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the number of cores for parallel processing\n",
    "n_cores = max(1, os.cpu_count() // 2)\n",
    "print(f\"Using {n_cores} CPU cores for parallel processing\")\n",
    "\n",
    "# ====================== LOAD AND PREPROCESS DATA ======================\n",
    "print(\"\\n=== Loading and preprocessing data ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/N300_G69_transposed.csv\", dtype={'ssn':'Int64', 'type_of_attack': 'Int64', 'gen_attacked': 'Int64'})\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "\n",
    "# Create binary target column if not already present\n",
    "if 'attack' not in df.columns:\n",
    "    df['attack'] = (df['type_of_attack'] != 0).astype(int)  # 1 for attack, 0 for no attack\n",
    "\n",
    "# Display class distribution\n",
    "attack_count = df['attack'].sum()\n",
    "total_records = len(df)\n",
    "normal_count = total_records - attack_count\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Number of attacks: {attack_count}\")\n",
    "print(f\"Number of normal operations: {normal_count}\")\n",
    "print(f\"Attack percentage: {attack_count/total_records:.2%}\")\n",
    "\n",
    "# Distribution of attack types\n",
    "if 'type_of_attack' in df.columns:\n",
    "    attack_types = df[df['attack'] == 1]['type_of_attack'].value_counts()\n",
    "    print(\"\\nAttack type distribution:\")\n",
    "    print(attack_types)\n",
    "    \n",
    "    # Map attack types to descriptive names\n",
    "    attack_type_names = {\n",
    "        0: \"Normal\",\n",
    "        1: \"Ramp Rate Attack\",\n",
    "        2: \"Upper Limit Attack\",\n",
    "        3: \"Lower Limit Attack\",\n",
    "        4: \"Generation Cost Attack\"\n",
    "    }\n",
    "    \n",
    "    # Print distribution with names\n",
    "    for attack_type, count in attack_types.items():\n",
    "        print(f\"{attack_type_names.get(attack_type, 'Unknown')}: {count} instances\")\n",
    "\n",
    "# Distribution of attacked generators\n",
    "if 'gen_attacked' in df.columns:\n",
    "    attacked_gens = df[df['attack'] == 1]['gen_attacked'].value_counts()\n",
    "    print(\"\\nTop 10 attacked generators:\")\n",
    "    print(attacked_gens.head(10))\n",
    "\n",
    "\n",
    "# ====================== ADVANCED FEATURE ENGINEERING ======================\n",
    "# Apply the advanced feature engineering \n",
    "from advanced_features import engineer_advanced_features\n",
    "df = engineer_advanced_features(df)\n",
    "\n",
    "# ====================== FEATURE SELECTION AND MODELING ======================\n",
    "print(\"\\n=== Preparing features for modeling ===\")\n",
    "\n",
    "# Define feature columns, excluding the target and direct identifiers\n",
    "feature_columns = [col for col in df.columns if col not in ['attack', 'type_of_attack', 'gen_attacked']]\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df['attack']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Remove low variance features\n",
    "print(\"\\nApplying variance threshold...\")\n",
    "variance_threshold = 0.01\n",
    "selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_train_var = selector.fit_transform(X_train)\n",
    "X_test_var = selector.transform(X_test)\n",
    "\n",
    "print(f\"Features after variance thresholding: {X_train_var.shape[1]}\")\n",
    "\n",
    "# Feature selection using mutual information\n",
    "print(\"\\nSelecting most informative features...\")\n",
    "select_k = SelectKBest(mutual_info_classif, k=min(100, X_train_var.shape[1]))\n",
    "X_train_selected = select_k.fit_transform(X_train_var, y_train)\n",
    "X_test_selected = select_k.transform(X_test_var)\n",
    "\n",
    "print(f\"Features after selection: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# Print top 20 feature names\n",
    "selected_indices = select_k.get_support(indices=True)\n",
    "original_indices = selector.get_support(indices=True)\n",
    "selected_names = [X.columns[original_indices[i]] for i in selected_indices[:20]]\n",
    "scores = select_k.scores_[selected_indices]\n",
    "\n",
    "print(\"\\nTop 20 most informative features:\")\n",
    "for name, score in sorted(zip(selected_names, scores), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{name}: {score:.6f}\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# ====================== TRAIN MULTIPLE MODELS ======================\n",
    "print(\"\\n=== Training and evaluating models ===\")\n",
    "\n",
    "# Define a function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Attack'],\n",
    "                yticklabels=['Normal', 'Attack'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.savefig(f'{model_name.replace(\" \", \"_\")}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC curve and AUC (if applicable)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        try:\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                    label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve - {model_name}')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(f'{model_name.replace(\" \", \"_\")}_roc_curve.png')\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate ROC AUC: {e}\")\n",
    "            \n",
    "    # Feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_names = [X.columns[original_indices[i]] for i in selected_indices]\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Plot top 20 feature importances\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(f'Top 20 Feature Importances - {model_name}')\n",
    "        plt.bar(range(min(20, len(importances))), \n",
    "                importances[indices[:20]], align='center')\n",
    "        plt.xticks(range(min(20, len(importances))), \n",
    "                   [feature_names[i] for i in indices[:20]], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_name.replace(\" \", \"_\")}_feature_importance.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"\\nTop 10 most important features:\")\n",
    "        for i in range(min(10, len(importances))):\n",
    "            print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.6f}\")\n",
    "    \n",
    "    return model, accuracy, f1\n",
    "\n",
    "# ====================== MODEL 1: EXTRA TREES CLASSIFIER ======================\n",
    "print(\"\\n1. Training Extra Trees Classifier\")\n",
    "\n",
    "# Define parameter grid\n",
    "et_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "et_base = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "et_cv = RandomizedSearchCV(\n",
    "    et_base, et_param_grid, n_iter=200, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "et_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "et_best = et_cv.best_estimator_\n",
    "print(f\"Best parameters: {et_cv.best_params_}\")\n",
    "\n",
    "# Evaluate\n",
    "et_model, et_accuracy, et_f1 = evaluate_model(\n",
    "    et_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Extra Trees\"\n",
    ")\n",
    "\n",
    "# ====================== MODEL 2: GRADIENT BOOSTING ======================\n",
    "print(\"\\n2. Training Gradient Boosting Classifier\")\n",
    "\n",
    "# Define parameter grid\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "gb_base = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "gb_cv = RandomizedSearchCV(\n",
    "    gb_base, gb_param_grid, n_iter=200, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "gb_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "gb_best = gb_cv.best_estimator_\n",
    "print(f\"Best parameters: {gb_cv.best_params_}\")\n",
    "\n",
    "# Evaluate\n",
    "gb_model, gb_accuracy, gb_f1 = evaluate_model(\n",
    "    gb_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Gradient Boosting\"\n",
    ")\n",
    "\n",
    "# ====================== MODEL 3: RANDOM FOREST ======================\n",
    "print(\"\\n3. Training Random Forest Classifier\")\n",
    "\n",
    "# Define parameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "rf_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "rf_cv = RandomizedSearchCV(\n",
    "    rf_base, rf_param_grid, n_iter=200, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "rf_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "rf_best = rf_cv.best_estimator_\n",
    "print(f\"Best parameters: {rf_cv.best_params_}\")\n",
    "\n",
    "# Evaluate\n",
    "rf_model, rf_accuracy, rf_f1 = evaluate_model(\n",
    "    rf_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Random Forest\"\n",
    ")\n",
    "\n",
    "# ====================== MODEL 4: XGBOOST ======================\n",
    "print(\"\\n4. Training XGBoost Classifier\")\n",
    "\n",
    "# Define parameter grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "xgb_base = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# RandomizedSearchCV\n",
    "xgb_cv = RandomizedSearchCV(\n",
    "    xgb_base, xgb_param_grid, n_iter=200, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "xgb_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "xgb_best = xgb_cv.best_estimator_\n",
    "print(f\"Best parameters: {xgb_cv.best_params_}\")\n",
    "\n",
    "# Evaluate\n",
    "xgb_model, xgb_accuracy, xgb_f1 = evaluate_model(\n",
    "    xgb_best, X_train_scaled, X_test_scaled, y_train, y_test, \"XGBoost\"\n",
    ")\n",
    "\n",
    "# ====================== MODEL 5: LOGISTIC REGRESSION ======================\n",
    "print(\"\\n5. Training Logistic Regression\")\n",
    "\n",
    "# Define parameter grid\n",
    "lr_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 7),\n",
    "    'penalty': ['l1', 'l2', None],\n",
    "    'solver': ['liblinear', 'saga', 'lbfgs'],\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "lr_base = LogisticRegression(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "lr_cv = RandomizedSearchCV(\n",
    "    lr_base, lr_param_grid, n_iter=50, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "lr_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best model\n",
    "lr_best = lr_cv.best_estimator_\n",
    "print(f\"Best parameters: {lr_cv.best_params_}\")\n",
    "\n",
    "# Evaluate\n",
    "lr_model, lr_accuracy, lr_f1 = evaluate_model(\n",
    "    lr_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Logistic Regression\"\n",
    ")\n",
    "\n",
    "# ====================== ENSEMBLE MODEL: VOTING CLASSIFIER ======================\n",
    "print(\"\\n6. Training Voting Classifier Ensemble\")\n",
    "\n",
    "# Create a dictionary of our models\n",
    "models = {\n",
    "    'ExtraTrees': et_best,\n",
    "    'GradientBoosting': gb_best,\n",
    "    'RandomForest': rf_best,\n",
    "    'XGBoost': xgb_best,\n",
    "    'LogisticRegression': lr_best\n",
    "}\n",
    "\n",
    "# Calculate scores\n",
    "model_scores = {\n",
    "    'ExtraTrees': et_f1,\n",
    "    'GradientBoosting': gb_f1,\n",
    "    'RandomForest': rf_f1,\n",
    "    'XGBoost': xgb_f1,\n",
    "    'LogisticRegression': lr_f1\n",
    "}\n",
    "\n",
    "# Sort by F1 score and select top 3\n",
    "top_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "print(f\"Top 3 models for ensemble: {[model[0] for model in top_models]}\")\n",
    "\n",
    "# Create voting classifier with top 3 models\n",
    "estimators = [(name, models[name]) for name, _ in top_models]\n",
    "voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "# Evaluate\n",
    "voting_model, voting_accuracy, voting_f1 = evaluate_model(\n",
    "    voting_clf, X_train_scaled, X_test_scaled, y_train, y_test, \"Voting Classifier\"\n",
    ")\n",
    "\n",
    "# ====================== SUMMARY ======================\n",
    "print(\"\\n=== Model Performance Summary ===\")\n",
    "models_summary = {\n",
    "    'Extra Trees': (et_accuracy, et_f1),\n",
    "    'Gradient Boosting': (gb_accuracy, gb_f1),\n",
    "    'Random Forest': (rf_accuracy, rf_f1),\n",
    "    'XGBoost': (xgb_accuracy, xgb_f1),\n",
    "    'Logistic Regression': (lr_accuracy, lr_f1),\n",
    "    'Voting Classifier': (voting_accuracy, voting_f1)\n",
    "}\n",
    "\n",
    "# Sort by F1 score\n",
    "sorted_models = sorted(models_summary.items(), key=lambda x: x[1][1], reverse=True)\n",
    "\n",
    "print(\"\\nModels ranked by F1 score:\")\n",
    "for i, (model_name, (acc, f1)) in enumerate(sorted_models, 1):\n",
    "    print(f\"{i}. {model_name}: Accuracy = {acc:.4f}, F1 = {f1:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model_name = sorted_models[0][0]\n",
    "best_model = None\n",
    "\n",
    "if best_model_name == 'Extra Trees':\n",
    "    best_model = et_model\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_model = gb_model\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "elif best_model_name == 'XGBoost':\n",
    "    best_model = xgb_model\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    best_model = lr_model\n",
    "else:\n",
    "    best_model = voting_model\n",
    "\n",
    "import joblib\n",
    "joblib.dump(best_model, f'best_model_{best_model_name.replace(\" \", \"_\")}.pkl')\n",
    "print(f\"\\nBest model ({best_model_name}) saved as 'best_model_{best_model_name.replace(' ', '_')}.pkl'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe7486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing output files...\n",
      "Moved Extra_Trees_confusion_matrix.png to dfigures/\n",
      "Moved Extra_Trees_feature_importance.png to dfigures/\n",
      "Moved Extra_Trees_roc_curve.png to dfigures/\n",
      "Moved Gradient_Boosting_confusion_matrix.png to dfigures/\n",
      "Moved Gradient_Boosting_feature_importance.png to dfigures/\n",
      "Moved Gradient_Boosting_roc_curve.png to dfigures/\n",
      "Moved Logistic_Regression_confusion_matrix.png to dfigures/\n",
      "Moved Logistic_Regression_roc_curve.png to dfigures/\n",
      "Moved Random_Forest_confusion_matrix.png to dfigures/\n",
      "Moved Random_Forest_feature_importance.png to dfigures/\n",
      "Moved Random_Forest_roc_curve.png to dfigures/\n",
      "Moved Voting_Classifier_confusion_matrix.png to dfigures/\n",
      "Moved Voting_Classifier_roc_curve.png to dfigures/\n",
      "Moved XGBoost_confusion_matrix.png to dfigures/\n",
      "Moved XGBoost_feature_importance.png to dfigures/\n",
      "Moved XGBoost_roc_curve.png to dfigures/\n",
      "Moved best_model_Gradient_Boosting.pkl to models/\n",
      "Organization complete. 16 figures moved to 'dfigures' folder and 1 models moved to 'models' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def organize_output_files():\n",
    "    \"\"\"\n",
    "    Organize output files from the power grid attack detection model:\n",
    "    - Move all PNG files to a 'dfigures' folder\n",
    "    - Move all model files (.pkl) to a 'models' folder\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs('dfigures', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    print(\"Organizing output files...\")\n",
    "    \n",
    "    # Move PNG files to 'dfigures' folder\n",
    "    png_files = glob.glob('*.png')\n",
    "    for file in png_files:\n",
    "        dest_path = os.path.join('dfigures', file)\n",
    "        if os.path.exists(dest_path):\n",
    "            os.remove(dest_path)  # Remove if exists to avoid errors\n",
    "        shutil.move(file, 'dfigures')\n",
    "        print(f\"Moved {file} to dfigures/\")\n",
    "    \n",
    "    # Move model files to 'models' folder\n",
    "    model_files = glob.glob('*.pkl')\n",
    "    for file in model_files:\n",
    "        dest_path = os.path.join('models', file)\n",
    "        if os.path.exists(dest_path):\n",
    "            os.remove(dest_path)  # Remove if exists to avoid errors\n",
    "        shutil.move(file, 'models')\n",
    "        print(f\"Moved {file} to models/\")\n",
    "    \n",
    "    print(f\"Organization complete. {len(png_files)} figures moved to 'dfigures' folder and {len(model_files)} models moved to 'models' folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    organize_output_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
