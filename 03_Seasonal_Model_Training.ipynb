{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153bfadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 CPU cores for parallel processing\n",
      "\n",
      "=== Loading data ===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the number of cores for parallel processing\n",
    "n_cores = max(1, os.cpu_count() // 2)\n",
    "print(f\"Using {n_cores} CPU cores for parallel processing\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('models/session_models', exist_ok=True)\n",
    "os.makedirs('session_figures', exist_ok=True)  # Separate folder for figures\n",
    "os.makedirs('session_figures/confusion_matrices', exist_ok=True)\n",
    "os.makedirs('session_figures/roc_curves', exist_ok=True)\n",
    "os.makedirs('session_figures/feature_importance', exist_ok=True)\n",
    "os.makedirs('session_figures/summaries', exist_ok=True)\n",
    "\n",
    "# Define a function to train models for a specific session\n",
    "def train_session_model(df, session_id, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Train all five models on data from a specific session with RandomizedSearchCV\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The full dataset\n",
    "    session_id : int\n",
    "        The session ID to filter by\n",
    "    test_size : float\n",
    "        The proportion of data to use for testing (default: 0.1 for 90-10 split)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training models for Session {session_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Filter data for this session\n",
    "    session_df = df[df['ssn'] == session_id].copy()\n",
    "    print(f\"Session {session_id} dataset shape: {session_df.shape}\")\n",
    "    \n",
    "    # Create binary target column if not already present\n",
    "    if 'attack' not in session_df.columns:\n",
    "        session_df['attack'] = (session_df['type_of_attack'] != 0).astype(int)  # 1 for attack, 0 for no attack\n",
    "    \n",
    "    # Display class distribution\n",
    "    attack_count = session_df['attack'].sum()\n",
    "    total_records = len(session_df)\n",
    "    normal_count = total_records - attack_count\n",
    "    print(f\"Total records: {total_records}\")\n",
    "    print(f\"Number of attacks: {attack_count}\")\n",
    "    print(f\"Number of normal operations: {normal_count}\")\n",
    "    print(f\"Attack percentage: {attack_count/total_records:.2%}\")\n",
    "    \n",
    "    # If either class has too few samples, notify but continue\n",
    "    if attack_count < 5 or normal_count < 5:\n",
    "        print(f\"WARNING: Session {session_id} has very few samples in one class.\")\n",
    "    \n",
    "    # Define feature columns, excluding the target and direct identifiers\n",
    "    feature_columns = [col for col in session_df.columns if col not in ['attack', 'type_of_attack', 'gen_attacked']]\n",
    "    \n",
    "    X = session_df[feature_columns]\n",
    "    y = session_df['attack']\n",
    "    \n",
    "    # Split dataset into training and testing sets (90-10 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=None)\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Feature selection and preprocessing pipeline\n",
    "    # 1. Remove low variance features\n",
    "    print(\"\\nApplying variance threshold...\")\n",
    "    variance_threshold = 0.01\n",
    "    selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    X_train_var = selector.fit_transform(X_train)\n",
    "    X_test_var = selector.transform(X_test)\n",
    "    print(f\"Features after variance thresholding: {X_train_var.shape[1]}\")\n",
    "    \n",
    "    # 2. Feature selection using mutual information\n",
    "    print(\"\\nSelecting most informative features...\")\n",
    "    select_k = SelectKBest(mutual_info_classif, k=min(100, X_train_var.shape[1]))\n",
    "    X_train_selected = select_k.fit_transform(X_train_var, y_train)\n",
    "    X_test_selected = select_k.transform(X_test_var)\n",
    "    print(f\"Features after selection: {X_train_selected.shape[1]}\")\n",
    "    \n",
    "    # Print top 20 feature names\n",
    "    selected_indices = select_k.get_support(indices=True)\n",
    "    original_indices = selector.get_support(indices=True)\n",
    "    selected_names = [X.columns[original_indices[i]] for i in selected_indices[:20]]\n",
    "    scores = select_k.scores_[selected_indices]\n",
    "    \n",
    "    print(\"\\nTop 20 most informative features:\")\n",
    "    for name, score in sorted(zip(selected_names, scores), key=lambda x: x[1], reverse=True)[:20]:\n",
    "        print(f\"{name}: {score:.6f}\")\n",
    "    \n",
    "    # 3. Scale features\n",
    "    print(\"\\nScaling features...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "    \n",
    "    # Define a function to evaluate models\n",
    "    def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "        # Create session-specific filename prefix\n",
    "        prefix = f\"ssn{session_id}_{model_name.replace(' ', '_').lower()}\"\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['Normal', 'Attack'],\n",
    "                    yticklabels=['Normal', 'Attack'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(f'Session {session_id} - Confusion Matrix - {model_name}')\n",
    "        plt.savefig(f'session_figures/confusion_matrices/{prefix}_confusion_matrix.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # ROC curve and AUC (if applicable)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            try:\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "                \n",
    "                # Plot ROC curve\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                        label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'Session {session_id} - ROC Curve - {model_name}')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.savefig(f'session_figures/roc_curves/{prefix}_roc_curve.png')\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not calculate ROC AUC: {e}\")\n",
    "                \n",
    "        # Feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_names = [X.columns[original_indices[i]] for i in selected_indices]\n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            # Plot top 20 feature importances\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.title(f'Session {session_id} - Top 20 Feature Importances - {model_name}')\n",
    "            n_features = min(20, len(importances))\n",
    "            plt.bar(range(n_features), \n",
    "                    importances[indices[:n_features]], align='center')\n",
    "            plt.xticks(range(n_features), \n",
    "                    [feature_names[i] for i in indices[:n_features]], rotation=90)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'session_figures/feature_importance/{prefix}_feature_importance.png')\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"\\nTop 10 most important features:\")\n",
    "            for i in range(min(10, len(importances))):\n",
    "                print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.6f}\")\n",
    "        \n",
    "        return model, accuracy, f1\n",
    "\n",
    "    # ====================== MODEL 1: EXTRA TREES CLASSIFIER ======================\n",
    "    print(\"\\n1. Training Extra Trees Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    et_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    et_base = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    et_cv = RandomizedSearchCV(\n",
    "        et_base, et_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    et_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    et_best = et_cv.best_estimator_\n",
    "    print(f\"Best parameters: {et_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    et_model, et_accuracy, et_f1 = evaluate_model(\n",
    "        et_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Extra Trees\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 2: GRADIENT BOOSTING ======================\n",
    "    print(\"\\n2. Training Gradient Boosting Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    gb_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    gb_base = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    gb_cv = RandomizedSearchCV(\n",
    "        gb_base, gb_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    gb_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    gb_best = gb_cv.best_estimator_\n",
    "    print(f\"Best parameters: {gb_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    gb_model, gb_accuracy, gb_f1 = evaluate_model(\n",
    "        gb_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Gradient Boosting\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 3: RANDOM FOREST ======================\n",
    "    print(\"\\n3. Training Random Forest Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    rf_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    rf_cv = RandomizedSearchCV(\n",
    "        rf_base, rf_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    rf_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    rf_best = rf_cv.best_estimator_\n",
    "    print(f\"Best parameters: {rf_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    rf_model, rf_accuracy, rf_f1 = evaluate_model(\n",
    "        rf_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Random Forest\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 4: XGBOOST ======================\n",
    "    print(\"\\n4. Training XGBoost Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3],\n",
    "        'gamma': [0, 0.1],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    xgb_base = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    xgb_cv = RandomizedSearchCV(\n",
    "        xgb_base, xgb_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    xgb_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    xgb_best = xgb_cv.best_estimator_\n",
    "    print(f\"Best parameters: {xgb_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    xgb_model, xgb_accuracy, xgb_f1 = evaluate_model(\n",
    "        xgb_best, X_train_scaled, X_test_scaled, y_train, y_test, \"XGBoost\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 5: LOGISTIC REGRESSION ======================\n",
    "    print(\"\\n5. Training Logistic Regression for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    lr_param_grid = {\n",
    "        'C': np.logspace(-3, 3, 7),\n",
    "        'penalty': ['l1', 'l2', None],\n",
    "        'solver': ['liblinear', 'saga', 'lbfgs'],\n",
    "        'max_iter': [1000, 2000]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    lr_base = LogisticRegression(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    lr_cv = RandomizedSearchCV(\n",
    "        lr_base, lr_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    lr_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    lr_best = lr_cv.best_estimator_\n",
    "    print(f\"Best parameters: {lr_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    lr_model, lr_accuracy, lr_f1 = evaluate_model(\n",
    "        lr_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Logistic Regression\"\n",
    "    )\n",
    "\n",
    "    # ====================== ENSEMBLE MODEL: VOTING CLASSIFIER ======================\n",
    "    print(\"\\n6. Training Voting Classifier Ensemble for Session\", session_id)\n",
    "\n",
    "    # Create a dictionary of our models\n",
    "    models = {\n",
    "        'ExtraTrees': et_best,\n",
    "        'GradientBoosting': gb_best,\n",
    "        'RandomForest': rf_best,\n",
    "        'XGBoost': xgb_best,\n",
    "        'LogisticRegression': lr_best\n",
    "    }\n",
    "\n",
    "    # Calculate scores\n",
    "    model_scores = {\n",
    "        'ExtraTrees': et_f1,\n",
    "        'GradientBoosting': gb_f1,\n",
    "        'RandomForest': rf_f1,\n",
    "        'XGBoost': xgb_f1,\n",
    "        'LogisticRegression': lr_f1\n",
    "    }\n",
    "\n",
    "    # Sort by F1 score and select top 3\n",
    "    top_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(f\"Top 3 models for ensemble: {[model[0] for model in top_models]}\")\n",
    "\n",
    "    # Create voting classifier with top 3 models\n",
    "    estimators = [(name, models[name]) for name, _ in top_models]\n",
    "    voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "    # Evaluate\n",
    "    voting_model, voting_accuracy, voting_f1 = evaluate_model(\n",
    "        voting_clf, X_train_scaled, X_test_scaled, y_train, y_test, \"Voting Classifier\"\n",
    "    )\n",
    "\n",
    "    # ====================== SUMMARY ======================\n",
    "    print(\"\\n=== Model Performance Summary for Session\", session_id, \"===\")\n",
    "    models_summary = {\n",
    "        'Extra Trees': (et_accuracy, et_f1),\n",
    "        'Gradient Boosting': (gb_accuracy, gb_f1),\n",
    "        'Random Forest': (rf_accuracy, rf_f1),\n",
    "        'XGBoost': (xgb_accuracy, xgb_f1),\n",
    "        'Logistic Regression': (lr_accuracy, lr_f1),\n",
    "        'Voting Classifier': (voting_accuracy, voting_f1)\n",
    "    }\n",
    "\n",
    "    # Sort by F1 score\n",
    "    sorted_models = sorted(models_summary.items(), key=lambda x: x[1][1], reverse=True)\n",
    "\n",
    "    print(\"\\nModels ranked by F1 score:\")\n",
    "    for i, (model_name, (acc, f1)) in enumerate(sorted_models, 1):\n",
    "        print(f\"{i}. {model_name}: Accuracy = {acc:.4f}, F1 = {f1:.4f}\")\n",
    "\n",
    "    # Plot model comparison (both Accuracy and F1 Score)\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    model_names = [name for name, _ in sorted_models]\n",
    "    accuracies = [acc for _, (acc, _) in sorted_models]\n",
    "    f1_scores = [f1 for _, (_, f1) in sorted_models]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='skyblue')\n",
    "    rects2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='salmon')\n",
    "    \n",
    "    ax.set_xlabel('Models', fontsize=14)\n",
    "    ax.set_ylabel('Scores', fontsize=14)\n",
    "    ax.set_title(f'Session {session_id} - Model Performance Comparison', fontsize=16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=12)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_ylim(0, 1.1)  # Ensure consistent y-axis scale\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for rect in rects1:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    for rect in rects2:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Add a grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'session_figures/summaries/ssn{session_id}_model_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save the best model\n",
    "    best_model_name = sorted_models[0][0]\n",
    "    best_model = None\n",
    "\n",
    "    if best_model_name == 'Extra Trees':\n",
    "        best_model = et_model\n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        best_model = gb_model\n",
    "    elif best_model_name == 'Random Forest':\n",
    "        best_model = rf_model\n",
    "    elif best_model_name == 'XGBoost':\n",
    "        best_model = xgb_model\n",
    "    elif best_model_name == 'Logistic Regression':\n",
    "        best_model = lr_model\n",
    "    else:\n",
    "        best_model = voting_model\n",
    "\n",
    "    # Save model package\n",
    "    model_package = {\n",
    "        'variance_selector': selector,\n",
    "        'feature_selector': select_k,\n",
    "        'scaler': scaler,\n",
    "        'model': best_model,\n",
    "        'feature_columns': feature_columns,\n",
    "        'selected_indices': selected_indices,\n",
    "        'original_indices': original_indices,\n",
    "        'best_model_name': best_model_name,\n",
    "        'metrics': {'accuracy': sorted_models[0][1][0], 'f1': sorted_models[0][1][1]},\n",
    "        'all_models': {\n",
    "            'ExtraTrees': et_model,\n",
    "            'GradientBoosting': gb_model,\n",
    "            'RandomForest': rf_model,\n",
    "            'XGBoost': xgb_model,\n",
    "            'LogisticRegression': lr_model,\n",
    "            'VotingClassifier': voting_model\n",
    "        },\n",
    "        'all_metrics': models_summary\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/session_models/ssn{session_id}_{best_model_name.replace(' ', '_').lower()}.pkl\"\n",
    "    joblib.dump(model_package, model_path)\n",
    "    print(f\"\\nBest model for Session {session_id} ({best_model_name}) saved as '{model_path}'\")\n",
    "    \n",
    "    print(f\"\\nSession {session_id} training completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return model_package\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n=== Loading data ===\")\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(\"data/N300_G69_transposed.csv\", dtype={'ssn':'Int64', 'type_of_attack': 'Int64', 'gen_attacked': 'Int64'})\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    if 'attack' not in df.columns:\n",
    "        df['attack'] = (df['type_of_attack'] != 0).astype(int)  # 1 for attack, 0 for no attack\n",
    "    \n",
    "    \n",
    "    # ====================== ADVANCED FEATURE ENGINEERING ======================\n",
    "    # Apply the advanced feature engineering \n",
    "    from advanced_features import engineer_advanced_features\n",
    "    df = engineer_advanced_features(df)\n",
    "        \n",
    "    # Get unique session IDs\n",
    "    session_ids = df['ssn'].unique()\n",
    "    print(f\"Found {len(session_ids)} unique sessions: {session_ids}\")\n",
    "    \n",
    "    # Train models for each session\n",
    "    session_results = {}\n",
    "    \n",
    "    for session_id in session_ids:\n",
    "        model_package = train_session_model(df, session_id)\n",
    "        session_results[session_id] = model_package['metrics']['f1']\n",
    "    \n",
    "    # Create a summary of results across sessions\n",
    "    print(\"\\n=== Session Models Summary ===\")\n",
    "    print(\"\\nModel performance by session:\")\n",
    "    \n",
    "    # Table format for terminal display\n",
    "    headers = [\"Session\", \"Best Model\", \"Accuracy\", \"F1 Score\"]\n",
    "    rows = []\n",
    "    \n",
    "    # Dictionary to store accuracy and F1 scores for each session\n",
    "    accuracy_by_session = {}\n",
    "    f1_by_session = {}\n",
    "    best_models_by_session = {}\n",
    "   # Fixed version - only show the best model per session\n",
    "    for session_id in session_ids:\n",
    "        best_model_file = None\n",
    "        # Find the file that corresponds to the best model for this session\n",
    "        for file in os.listdir(\"models/session_models/\"):\n",
    "            if file.startswith(f\"ssn{session_id}_\") and not file.endswith(\"_noscale.pkl\"):\n",
    "                # If we already found a file for this session, we need to determine which is newer/correct\n",
    "                if best_model_file is None:\n",
    "                    best_model_file = file\n",
    "                \n",
    "        if best_model_file:\n",
    "            model_path = os.path.join(\"models/session_models/\", best_model_file)\n",
    "            model_package = joblib.load(model_path)\n",
    "            \n",
    "            accuracy = model_package['metrics']['accuracy']\n",
    "            f1 = model_package['metrics']['f1']\n",
    "            best_model = model_package['best_model_name']\n",
    "            \n",
    "            accuracy_by_session[session_id] = accuracy\n",
    "            f1_by_session[session_id] = f1\n",
    "            best_models_by_session[session_id] = best_model\n",
    "            \n",
    "            # Only add one row per session\n",
    "            rows.append([str(session_id), str(best_model), f\"{accuracy:.4f}\", f\"{f1:.4f}\"])\n",
    "    \n",
    "    # Print table\n",
    "    col_width = max(len(word) for row in [headers] + rows for word in row) + 2\n",
    "    print(\"+\" + \"+\".join([\"-\" * col_width for _ in headers]) + \"+\")\n",
    "    print(\"|\" + \"|\".join(word.ljust(col_width) for word in headers) + \"|\")\n",
    "    print(\"+\" + \"+\".join([\"-\" * col_width for _ in headers]) + \"+\")\n",
    "    for row in rows:\n",
    "        print(\"|\" + \"|\".join(str(word).ljust(col_width) for word in row) + \"|\")\n",
    "    print(\"+\" + \"+\".join([\"-\" * col_width for _ in headers]) + \"+\")\n",
    "    \n",
    "    # Identify best and worst performing sessions by both metrics\n",
    "    best_acc_session = max(accuracy_by_session.items(), key=lambda x: x[1])\n",
    "    worst_acc_session = min(accuracy_by_session.items(), key=lambda x: x[1])\n",
    "    best_f1_session = max(f1_by_session.items(), key=lambda x: x[1])\n",
    "    worst_f1_session = min(f1_by_session.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\nBest performing session (Accuracy): Session {best_acc_session[0]} with {best_acc_session[1]:.4f}\")\n",
    "    print(f\"Worst performing session (Accuracy): Session {worst_acc_session[0]} with {worst_acc_session[1]:.4f}\")\n",
    "    print(f\"Best performing session (F1 Score): Session {best_f1_session[0]} with {best_f1_session[1]:.4f}\")\n",
    "    print(f\"Worst performing session (F1 Score): Session {worst_f1_session[0]} with {worst_f1_session[1]:.4f}\")\n",
    "    \n",
    "    # Plot summary of scores by session\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    sessions = list(sorted(accuracy_by_session.keys()))\n",
    "    accuracies = [accuracy_by_session[s] for s in sessions]\n",
    "    f1_scores = [f1_by_session[s] for s in sessions]\n",
    "    \n",
    "    x = np.arange(len(sessions))\n",
    "    width = 0.35\n",
    "    \n",
    "    rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='skyblue')\n",
    "    rects2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='salmon')\n",
    "    \n",
    "    ax.set_xlabel('Session ID', fontsize=14)\n",
    "    ax.set_ylabel('Score', fontsize=14)\n",
    "    ax.set_title('Attack Detection Performance by Session', fontsize=16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'Session {s}' for s in sessions], fontsize=12)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for rect in rects1:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "                    \n",
    "    for rect in rects2:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/session_performance_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot best model by session\n",
    "    model_counts = {}\n",
    "    for session_id in session_ids:\n",
    "        files = os.listdir(\"models/session_models/\")\n",
    "        for file in files:\n",
    "            if file.startswith(f\"ssn{session_id}_\"):\n",
    "                model_name = file.replace(f\"ssn{session_id}_\", \"\").replace(\".pkl\", \"\").replace(\"_\", \" \").title()\n",
    "                if model_name not in model_counts:\n",
    "                    model_counts[model_name] = 0\n",
    "                model_counts[model_name] += 1\n",
    "    \n",
    "    # Create a bar chart of best model types\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    models = list(model_counts.keys())\n",
    "    counts = [model_counts[m] for m in models]\n",
    "    \n",
    "    plt.bar(models, counts, color='lightgreen')\n",
    "    plt.xlabel('Model Type', fontsize=14)\n",
    "    plt.ylabel('Number of Sessions', fontsize=14)\n",
    "    plt.title('Best Model Type by Number of Sessions', fontsize=16)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for i, v in enumerate(counts):\n",
    "        plt.text(i, v + 0.1, str(v), ha='center', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/best_model_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create comparison table for all models across all sessions\n",
    "    print(\"\\n=== Creating cross-session model comparison ===\")\n",
    "    \n",
    "    # Dictionary to store F1 scores for each model type across sessions\n",
    "    model_performance = {\n",
    "        'Extra Trees': {},\n",
    "        'Gradient Boosting': {},\n",
    "        'Random Forest': {},\n",
    "        'XGBoost': {},\n",
    "        'Logistic Regression': {},\n",
    "        'Voting Classifier': {}\n",
    "    }\n",
    "    \n",
    "    # Load model packages to get performance metrics\n",
    "    for session_id in session_ids:\n",
    "        for file in os.listdir(\"models/session_models/\"):\n",
    "            if file.startswith(f\"ssn{session_id}_\"):\n",
    "                model_path = os.path.join(\"models/session_models/\", file)\n",
    "                model_package = joblib.load(model_path)\n",
    "                \n",
    "                # Extract all model metrics\n",
    "                for model_name, (acc, f1) in model_package['all_metrics'].items():\n",
    "                    model_performance[model_name][session_id] = f1\n",
    "    \n",
    "    # Create a heatmap of model performance across sessions\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Initialize data structures for both metrics\n",
    "    accuracy_heatmap_data = []\n",
    "    f1_heatmap_data = []\n",
    "    model_names = list(model_performance.keys())\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        # Collect accuracy scores\n",
    "        accuracy_scores = []\n",
    "        f1_scores = []\n",
    "        for session_id in session_ids:\n",
    "            # For each session, load the model package and get metrics for all models\n",
    "            found = False\n",
    "            for file in os.listdir(\"models/session_models/\"):\n",
    "                if file.startswith(f\"ssn{session_id}_\"):\n",
    "                    model_path = os.path.join(\"models/session_models/\", file)\n",
    "                    model_package = joblib.load(model_path)\n",
    "                    \n",
    "                    # Extract metrics for this model\n",
    "                    if model_name in model_package['all_metrics']:\n",
    "                        acc, f1 = model_package['all_metrics'][model_name]\n",
    "                        accuracy_scores.append(acc)\n",
    "                        f1_scores.append(f1)\n",
    "                        found = True\n",
    "                        break\n",
    "            \n",
    "            if not found:\n",
    "                accuracy_scores.append(0)\n",
    "                f1_scores.append(0)\n",
    "                \n",
    "        accuracy_heatmap_data.append(accuracy_scores)\n",
    "        f1_heatmap_data.append(f1_scores)\n",
    "    \n",
    "    # Create accuracy heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(accuracy_heatmap_data, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", \n",
    "                xticklabels=[f\"Session {s}\" for s in session_ids],\n",
    "                yticklabels=model_names,\n",
    "                vmin=0, vmax=1)\n",
    "    \n",
    "    plt.title('Accuracy by Model and Session', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/model_session_accuracy_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create F1 score heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(f1_heatmap_data, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", \n",
    "                xticklabels=[f\"Session {s}\" for s in session_ids],\n",
    "                yticklabels=model_names,\n",
    "                vmin=0, vmax=1)\n",
    "    \n",
    "    plt.title('F1 Score by Model and Session', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/model_session_f1_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create attack type distribution visualization\n",
    "    if 'type_of_attack' in df.columns:\n",
    "        attack_types = df[df['attack'] == 1]['type_of_attack'].value_counts()\n",
    "        \n",
    "        # Map attack types to descriptive names\n",
    "        attack_type_names = {\n",
    "            0: \"Normal\",\n",
    "            1: \"Ramp Rate Attack\",\n",
    "            2: \"Upper Limit Attack\",\n",
    "            3: \"Lower Limit Attack\",\n",
    "            4: \"Generation Cost Attack\"\n",
    "        }\n",
    "        \n",
    "        # Create a separate dataset for attack type distribution by session\n",
    "        attack_by_session = {}\n",
    "        for session_id in session_ids:\n",
    "            session_attacks = df[(df['ssn'] == session_id) & (df['attack'] == 1)]['type_of_attack'].value_counts()\n",
    "            attack_by_session[session_id] = {attack_type_names.get(k, f\"Type {k}\"): v \n",
    "                                           for k, v in session_attacks.items()}\n",
    "        \n",
    "        # Prepare data for stacked bar chart\n",
    "        attack_types_data = []\n",
    "        for attack_type in range(1, 5):  # Attack types 1-4\n",
    "            type_data = []\n",
    "            for session_id in session_ids:\n",
    "                if attack_type_names.get(attack_type) in attack_by_session[session_id]:\n",
    "                    type_data.append(attack_by_session[session_id][attack_type_names.get(attack_type)])\n",
    "                else:\n",
    "                    type_data.append(0)\n",
    "            attack_types_data.append(type_data)\n",
    "        \n",
    "        # Create stacked bar chart\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        bottom = np.zeros(len(session_ids))\n",
    "        \n",
    "        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "        \n",
    "        for i, attack_data in enumerate(attack_types_data):\n",
    "            plt.bar([f\"Session {s}\" for s in session_ids], attack_data, bottom=bottom, \n",
    "                   label=attack_type_names.get(i+1), color=colors[i])\n",
    "            bottom += attack_data\n",
    "        \n",
    "        plt.xlabel('Session', fontsize=14)\n",
    "        plt.ylabel('Number of Attacks', fontsize=14)\n",
    "        plt.title('Attack Type Distribution by Session', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.xticks(rotation=0, fontsize=12)\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('session_figures/summaries/attack_type_distribution_by_session.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Create pie charts for each session's attack distribution\n",
    "        for session_id in session_ids:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            session_data = df[(df['ssn'] == session_id) & (df['attack'] == 1)]\n",
    "            \n",
    "            if len(session_data) > 0:\n",
    "                attack_counts = session_data['type_of_attack'].value_counts()\n",
    "                labels = [attack_type_names.get(i, f\"Type {i}\") for i in attack_counts.index]\n",
    "                \n",
    "                plt.pie(attack_counts, labels=labels, autopct='%1.1f%%', \n",
    "                       startangle=90, colors=colors, wedgeprops={'edgecolor': 'black'})\n",
    "                plt.axis('equal')\n",
    "                plt.title(f'Session {session_id} - Attack Type Distribution', fontsize=16)\n",
    "                plt.savefig(f'session_figures/summaries/session{session_id}_attack_distribution_pie.png')\n",
    "                plt.close()\n",
    "    \n",
    "    print(f\"\\nAll session-specific models trained in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Results summary saved to 'session_figures/summaries/' directory\")\n",
    "    print(f\"Individual session figures saved to:\")\n",
    "    print(f\"  - 'session_figures/confusion_matrices/' for confusion matrices\")\n",
    "    print(f\"  - 'session_figures/roc_curves/' for ROC curves\")\n",
    "    print(f\"  - 'session_figures/feature_importance/' for feature importance plots\")\n",
    "    print(f\"All models saved to 'models/session_models/' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
