{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153bfadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 CPU cores for parallel processing\n",
      "\n",
      "=== Loading data ===\n",
      "Dataset loaded with shape: (1920, 34036)\n",
      "\n",
      "=== Performing domain-specific feature engineering ===\n",
      "\n",
      "=== Performing advanced feature engineering ===\n",
      "\n",
      "1. Creating specialized Lower Limit Attack detection features...\n",
      "3. Creating features for large changes without ramp constraint activation...\n",
      "4. Creating temporal pattern detection features...\n",
      "5. Creating composite attack-specific features...\n",
      "6. Creating interaction features with new composite features...\n",
      "Created 18 new interaction features\n",
      "\n",
      "Advanced feature engineering complete. Added 65 new features.\n",
      "New DataFrame shape: (1920, 34137)\n",
      "Found 4 unique sessions: <IntegerArray>\n",
      "[1, 2, 3, 4]\n",
      "Length: 4, dtype: Int64\n",
      "\n",
      "================================================================================\n",
      "Training models for Session 1\n",
      "================================================================================\n",
      "Session 1 dataset shape: (480, 34137)\n",
      "Total records: 480\n",
      "Number of attacks: 240\n",
      "Number of normal operations: 240\n",
      "Attack percentage: 50.00%\n",
      "Training set shape: (432, 34134)\n",
      "Test set shape: (48, 34134)\n",
      "\n",
      "Applying variance threshold...\n",
      "Features after variance thresholding: 13283\n",
      "\n",
      "Selecting most informative features...\n",
      "Features after selection: 100\n",
      "\n",
      "Top 20 most informative features:\n",
      "ft5800: 0.066032\n",
      "ft6628: 0.062841\n",
      "ft8766: 0.058292\n",
      "ft2340: 0.053178\n",
      "ft3051: 0.052875\n",
      "ft6645: 0.052652\n",
      "ft1322: 0.052603\n",
      "ft4210: 0.052173\n",
      "ft128: 0.051713\n",
      "ft9314: 0.048366\n",
      "ft4334: 0.046926\n",
      "ft2668: 0.046232\n",
      "ft6637: 0.045887\n",
      "ft8664: 0.045226\n",
      "ft3002: 0.044477\n",
      "ft2224: 0.044423\n",
      "ft199: 0.042882\n",
      "ft131: 0.042303\n",
      "ft2301: 0.040872\n",
      "ft6555: 0.038556\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "1. Training Extra Trees Classifier for Session 1\n",
      "Best parameters: {'n_estimators': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Extra Trees Results:\n",
      "Accuracy: 0.8750\n",
      "F1 Score: 0.8889\n",
      "Confusion Matrix:\n",
      "[[18  3]\n",
      " [ 3 24]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        21\n",
      "           1       0.89      0.89      0.89        27\n",
      "\n",
      "    accuracy                           0.88        48\n",
      "   macro avg       0.87      0.87      0.87        48\n",
      "weighted avg       0.88      0.88      0.88        48\n",
      "\n",
      "ROC AUC: 0.9753\n",
      "\n",
      "Top 10 most important features:\n",
      "ramp_constraint_active: 0.089496\n",
      "fval_cos_10d: 0.086605\n",
      "fval_cos_10d_interaction: 0.080545\n",
      "change_std_ratio_no_constraint: 0.071017\n",
      "large_change_no_constraint_magnitude: 0.048470\n",
      "large_change_no_constraint_score: 0.043552\n",
      "fval_change: 0.034525\n",
      "lower_limit_approach_rate: 0.034238\n",
      "fval_sin_20d: 0.030523\n",
      "day_in_ssn_x_large_change_no_constraint_score: 0.027995\n",
      "\n",
      "2. Training Gradient Boosting Classifier for Session 1\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 200, 'min_samples_split': 5, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change: 0.224783\n",
      "fval_diff_lag_1: 0.187456\n",
      "fval_change_x_lower_limit_attack_score: 0.098036\n",
      "fval_cos_10d_interaction: 0.093740\n",
      "fval_cos_10d: 0.076152\n",
      "fval_change_jerk: 0.075838\n",
      "fval_diff_lag_1_x_lower_limit_attack_score: 0.052664\n",
      "fval_diff_lag_1_x_weighted_attack_score: 0.049019\n",
      "fval_change_x_weighted_attack_score: 0.023771\n",
      "lower_limit_approach_rate: 0.013018\n",
      "\n",
      "3. Training Random Forest Classifier for Session 1\n",
      "Best parameters: {'n_estimators': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.7917\n",
      "F1 Score: 0.8077\n",
      "Confusion Matrix:\n",
      "[[17  4]\n",
      " [ 6 21]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.81      0.77        21\n",
      "           1       0.84      0.78      0.81        27\n",
      "\n",
      "    accuracy                           0.79        48\n",
      "   macro avg       0.79      0.79      0.79        48\n",
      "weighted avg       0.80      0.79      0.79        48\n",
      "\n",
      "ROC AUC: 0.9418\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change: 0.112077\n",
      "fval_diff_lag_1: 0.096989\n",
      "fval_diff_lag_1_x_lower_limit_attack_score: 0.066732\n",
      "fval_change_x_lower_limit_attack_score: 0.064529\n",
      "lower_limit_approach_rate: 0.052913\n",
      "fval_diff_lag_1_x_weighted_attack_score: 0.048270\n",
      "fval_change_x_weighted_attack_score: 0.042318\n",
      "fval_cos_10d: 0.035188\n",
      "fval_change_jerk: 0.035017\n",
      "ramp_constraint_active_x_weighted_attack_score: 0.034779\n",
      "\n",
      "4. Training XGBoost Classifier for Session 1\n",
      "Best parameters: {'subsample': 0.9, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change: 0.292910\n",
      "fval_diff_lag_1: 0.281209\n",
      "fval_change_x_lower_limit_attack_score: 0.093369\n",
      "fval_change_x_weighted_attack_score: 0.090696\n",
      "lower_limit_approach_rate: 0.042933\n",
      "fval_change_jerk: 0.035412\n",
      "fval_cos_10d: 0.033121\n",
      "ft23658: 0.013667\n",
      "ft1322: 0.012396\n",
      "ft6645: 0.012279\n",
      "\n",
      "5. Training Logistic Regression for Session 1\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 1000, 'C': 1.0}\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.8125\n",
      "F1 Score: 0.8364\n",
      "Confusion Matrix:\n",
      "[[16  5]\n",
      " [ 4 23]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78        21\n",
      "           1       0.82      0.85      0.84        27\n",
      "\n",
      "    accuracy                           0.81        48\n",
      "   macro avg       0.81      0.81      0.81        48\n",
      "weighted avg       0.81      0.81      0.81        48\n",
      "\n",
      "ROC AUC: 0.7919\n",
      "\n",
      "6. Training Voting Classifier Ensemble for Session 1\n",
      "Top 3 models for ensemble: ['GradientBoosting', 'XGBoost', 'ExtraTrees']\n",
      "\n",
      "Voting Classifier Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "=== Model Performance Summary for Session 1 ===\n",
      "\n",
      "Models ranked by F1 score:\n",
      "1. Gradient Boosting: Accuracy = 1.0000, F1 = 1.0000\n",
      "2. XGBoost: Accuracy = 1.0000, F1 = 1.0000\n",
      "3. Voting Classifier: Accuracy = 1.0000, F1 = 1.0000\n",
      "4. Extra Trees: Accuracy = 0.8750, F1 = 0.8889\n",
      "5. Logistic Regression: Accuracy = 0.8125, F1 = 0.8364\n",
      "6. Random Forest: Accuracy = 0.7917, F1 = 0.8077\n",
      "\n",
      "Best model for Session 1 (Gradient Boosting) saved as 'models/session_models/ssn1_gradient_boosting.pkl'\n",
      "\n",
      "Session 1 training completed in 520.47 seconds\n",
      "\n",
      "================================================================================\n",
      "Training models for Session 2\n",
      "================================================================================\n",
      "Session 2 dataset shape: (480, 34137)\n",
      "Total records: 480\n",
      "Number of attacks: 240\n",
      "Number of normal operations: 240\n",
      "Attack percentage: 50.00%\n",
      "Training set shape: (432, 34134)\n",
      "Test set shape: (48, 34134)\n",
      "\n",
      "Applying variance threshold...\n",
      "Features after variance thresholding: 13019\n",
      "\n",
      "Selecting most informative features...\n",
      "Features after selection: 100\n",
      "\n",
      "Top 20 most informative features:\n",
      "ft5628: 0.067704\n",
      "ft2968: 0.056337\n",
      "ft5872: 0.054403\n",
      "ft4331: 0.053368\n",
      "ft4511: 0.052570\n",
      "ft5929: 0.052123\n",
      "ft4420: 0.049965\n",
      "ft5770: 0.049589\n",
      "ft5838: 0.048642\n",
      "ft103: 0.046562\n",
      "ft3793: 0.045989\n",
      "ft4385: 0.045289\n",
      "ft971: 0.044966\n",
      "ft965: 0.043767\n",
      "ft4513: 0.041045\n",
      "ft4158: 0.040849\n",
      "ft4517: 0.039599\n",
      "ft192: 0.039322\n",
      "ft957: 0.038674\n",
      "ft1672: 0.037762\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "1. Training Extra Trees Classifier for Session 2\n",
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 10, 'bootstrap': False}\n",
      "\n",
      "Extra Trees Results:\n",
      "Accuracy: 0.9167\n",
      "F1 Score: 0.9231\n",
      "Confusion Matrix:\n",
      "[[20  1]\n",
      " [ 3 24]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91        21\n",
      "           1       0.96      0.89      0.92        27\n",
      "\n",
      "    accuracy                           0.92        48\n",
      "   macro avg       0.91      0.92      0.92        48\n",
      "weighted avg       0.92      0.92      0.92        48\n",
      "\n",
      "ROC AUC: 0.9859\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_cos_10d: 0.104094\n",
      "ramp_constraint_active: 0.086095\n",
      "large_change_no_constraint_magnitude: 0.074784\n",
      "fval_cos_10d_interaction: 0.069382\n",
      "large_change_no_constraint_score: 0.065090\n",
      "change_std_ratio_no_constraint: 0.064059\n",
      "day_in_ssn_x_large_change_no_constraint_score: 0.039572\n",
      "fval_change: 0.032165\n",
      "fval_cos_20d_interaction: 0.030842\n",
      "fval_diff_lag_1: 0.030645\n",
      "\n",
      "2. Training Gradient Boosting Classifier for Session 2\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "lower_limit_approach_rate: 0.599101\n",
      "fval_cos_10d: 0.101558\n",
      "fval_cos_10d_interaction: 0.066157\n",
      "fval_change_jerk: 0.048570\n",
      "fval_change_std_3d_x_weighted_attack_score: 0.017696\n",
      "fval_diff_lag_1_x_lower_limit_attack_score: 0.015040\n",
      "ft29652: 0.011934\n",
      "change_std_ratio_no_constraint: 0.011428\n",
      "ft32109: 0.011023\n",
      "ft4331: 0.009195\n",
      "\n",
      "3. Training Random Forest Classifier for Session 2\n",
      "Best parameters: {'n_estimators': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.8958\n",
      "F1 Score: 0.9057\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 3 24]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        21\n",
      "           1       0.92      0.89      0.91        27\n",
      "\n",
      "    accuracy                           0.90        48\n",
      "   macro avg       0.89      0.90      0.89        48\n",
      "weighted avg       0.90      0.90      0.90        48\n",
      "\n",
      "ROC AUC: 0.9753\n",
      "\n",
      "Top 10 most important features:\n",
      "lower_limit_approach_rate: 0.128613\n",
      "fval_diff_lag_1: 0.112171\n",
      "fval_diff_lag_1_x_weighted_attack_score: 0.070721\n",
      "fval_diff_lag_1_x_lower_limit_attack_score: 0.059847\n",
      "fval_change_x_lower_limit_attack_score: 0.051948\n",
      "fval_change: 0.049366\n",
      "fval_change_x_weighted_attack_score: 0.045699\n",
      "fval_cos_10d: 0.039127\n",
      "fval_change_x_large_change_no_constraint_score: 0.036204\n",
      "change_std_ratio_no_constraint: 0.033977\n",
      "\n",
      "4. Training XGBoost Classifier for Session 2\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "lower_limit_approach_rate: 0.346610\n",
      "fval_change: 0.182809\n",
      "fval_diff_lag_1: 0.126657\n",
      "fval_change_x_lower_limit_attack_score: 0.044019\n",
      "fval_change_jerk: 0.040041\n",
      "change_std_ratio_no_constraint: 0.031103\n",
      "fval_cos_10d: 0.030119\n",
      "fval_cos_10d_interaction: 0.027441\n",
      "ft17857: 0.018723\n",
      "ft30619: 0.014586\n",
      "\n",
      "5. Training Logistic Regression for Session 2\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 1000, 'C': 1.0}\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.8125\n",
      "F1 Score: 0.8302\n",
      "Confusion Matrix:\n",
      "[[17  4]\n",
      " [ 5 22]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79        21\n",
      "           1       0.85      0.81      0.83        27\n",
      "\n",
      "    accuracy                           0.81        48\n",
      "   macro avg       0.81      0.81      0.81        48\n",
      "weighted avg       0.81      0.81      0.81        48\n",
      "\n",
      "ROC AUC: 0.8519\n",
      "\n",
      "6. Training Voting Classifier Ensemble for Session 2\n",
      "Top 3 models for ensemble: ['GradientBoosting', 'XGBoost', 'ExtraTrees']\n",
      "\n",
      "Voting Classifier Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "=== Model Performance Summary for Session 2 ===\n",
      "\n",
      "Models ranked by F1 score:\n",
      "1. Gradient Boosting: Accuracy = 1.0000, F1 = 1.0000\n",
      "2. XGBoost: Accuracy = 1.0000, F1 = 1.0000\n",
      "3. Voting Classifier: Accuracy = 1.0000, F1 = 1.0000\n",
      "4. Extra Trees: Accuracy = 0.9167, F1 = 0.9231\n",
      "5. Random Forest: Accuracy = 0.8958, F1 = 0.9057\n",
      "6. Logistic Regression: Accuracy = 0.8125, F1 = 0.8302\n",
      "\n",
      "Best model for Session 2 (Gradient Boosting) saved as 'models/session_models/ssn2_gradient_boosting.pkl'\n",
      "\n",
      "Session 2 training completed in 461.57 seconds\n",
      "\n",
      "================================================================================\n",
      "Training models for Session 3\n",
      "================================================================================\n",
      "Session 3 dataset shape: (480, 34137)\n",
      "Total records: 480\n",
      "Number of attacks: 240\n",
      "Number of normal operations: 240\n",
      "Attack percentage: 50.00%\n",
      "Training set shape: (432, 34134)\n",
      "Test set shape: (48, 34134)\n",
      "\n",
      "Applying variance threshold...\n",
      "Features after variance thresholding: 13778\n",
      "\n",
      "Selecting most informative features...\n",
      "Features after selection: 100\n",
      "\n",
      "Top 20 most informative features:\n",
      "ft4519: 0.071520\n",
      "ft4214: 0.065398\n",
      "ft9723: 0.059544\n",
      "ft8765: 0.058689\n",
      "ft3758: 0.057153\n",
      "ft7052: 0.052822\n",
      "ft10118: 0.052610\n",
      "ft4203: 0.052004\n",
      "ft3642: 0.050942\n",
      "ft8061: 0.049641\n",
      "ft6922: 0.048676\n",
      "ft8466: 0.044611\n",
      "ft7255: 0.043980\n",
      "ft80: 0.043318\n",
      "ft2389: 0.043287\n",
      "ft4128: 0.042087\n",
      "ft10051: 0.041866\n",
      "ft970: 0.040720\n",
      "ft1516: 0.040438\n",
      "ft2786: 0.039749\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "1. Training Extra Trees Classifier for Session 3\n",
      "Best parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Extra Trees Results:\n",
      "Accuracy: 0.9375\n",
      "F1 Score: 0.9434\n",
      "Confusion Matrix:\n",
      "[[20  1]\n",
      " [ 2 25]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93        21\n",
      "           1       0.96      0.93      0.94        27\n",
      "\n",
      "    accuracy                           0.94        48\n",
      "   macro avg       0.94      0.94      0.94        48\n",
      "weighted avg       0.94      0.94      0.94        48\n",
      "\n",
      "ROC AUC: 0.9912\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_cos_10d: 0.110349\n",
      "fval_cos_10d_interaction: 0.088563\n",
      "ramp_constraint_active: 0.067006\n",
      "large_change_no_constraint_score: 0.058596\n",
      "large_change_no_constraint_magnitude: 0.043003\n",
      "fval_sin_20d: 0.037410\n",
      "fval_change: 0.033176\n",
      "fval_diff_lag_1: 0.031691\n",
      "fval_cos_20d: 0.031186\n",
      "day_in_ssn_x_large_change_no_constraint_score: 0.028661\n",
      "\n",
      "2. Training Gradient Boosting Classifier for Session 3\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 0.9583\n",
      "F1 Score: 0.9643\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        21\n",
      "           1       0.93      1.00      0.96        27\n",
      "\n",
      "    accuracy                           0.96        48\n",
      "   macro avg       0.97      0.95      0.96        48\n",
      "weighted avg       0.96      0.96      0.96        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change: 0.321664\n",
      "lower_limit_approach_rate: 0.179854\n",
      "fval_cos_10d_interaction: 0.154861\n",
      "fval_cos_10d: 0.149397\n",
      "fval_diff_lag_1: 0.110127\n",
      "fval_ar3_residual: 0.016656\n",
      "fval_change_std_3d_x_weighted_attack_score: 0.012207\n",
      "ft19347: 0.007883\n",
      "ft27204: 0.006008\n",
      "large_change_no_constraint_magnitude: 0.005436\n",
      "\n",
      "3. Training Random Forest Classifier for Session 3\n",
      "Best parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.8750\n",
      "F1 Score: 0.8889\n",
      "Confusion Matrix:\n",
      "[[18  3]\n",
      " [ 3 24]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        21\n",
      "           1       0.89      0.89      0.89        27\n",
      "\n",
      "    accuracy                           0.88        48\n",
      "   macro avg       0.87      0.87      0.87        48\n",
      "weighted avg       0.88      0.88      0.88        48\n",
      "\n",
      "ROC AUC: 0.9700\n",
      "\n",
      "Top 10 most important features:\n",
      "lower_limit_approach_rate: 0.090220\n",
      "fval_change_x_weighted_attack_score: 0.074228\n",
      "fval_diff_lag_1: 0.073128\n",
      "fval_change: 0.069883\n",
      "fval_diff_lag_1_x_lower_limit_attack_score: 0.061806\n",
      "fval_diff_lag_1_x_weighted_attack_score: 0.059058\n",
      "fval_change_x_lower_limit_attack_score: 0.043359\n",
      "fval_cos_10d_interaction: 0.040529\n",
      "fval_cos_10d: 0.039313\n",
      "large_change_no_constraint_magnitude: 0.031186\n",
      "\n",
      "4. Training XGBoost Classifier for Session 3\n",
      "Best parameters: {'subsample': 0.9, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_change_x_lower_limit_attack_score: 0.377178\n",
      "fval_change: 0.145773\n",
      "lower_limit_approach_rate: 0.121681\n",
      "fval_diff_lag_1: 0.108025\n",
      "ft970: 0.043335\n",
      "fval_ar3_residual: 0.039460\n",
      "weighted_attack_score: 0.030452\n",
      "fval_cos_10d: 0.029100\n",
      "ft9723: 0.014704\n",
      "fval_change_std_3d_x_large_change_no_constraint_score: 0.013851\n",
      "\n",
      "5. Training Logistic Regression for Session 3\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 1000, 'C': 1.0}\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.8125\n",
      "F1 Score: 0.8302\n",
      "Confusion Matrix:\n",
      "[[17  4]\n",
      " [ 5 22]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79        21\n",
      "           1       0.85      0.81      0.83        27\n",
      "\n",
      "    accuracy                           0.81        48\n",
      "   macro avg       0.81      0.81      0.81        48\n",
      "weighted avg       0.81      0.81      0.81        48\n",
      "\n",
      "ROC AUC: 0.8289\n",
      "\n",
      "6. Training Voting Classifier Ensemble for Session 3\n",
      "Top 3 models for ensemble: ['XGBoost', 'GradientBoosting', 'ExtraTrees']\n",
      "\n",
      "Voting Classifier Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "=== Model Performance Summary for Session 3 ===\n",
      "\n",
      "Models ranked by F1 score:\n",
      "1. XGBoost: Accuracy = 1.0000, F1 = 1.0000\n",
      "2. Voting Classifier: Accuracy = 1.0000, F1 = 1.0000\n",
      "3. Gradient Boosting: Accuracy = 0.9583, F1 = 0.9643\n",
      "4. Extra Trees: Accuracy = 0.9375, F1 = 0.9434\n",
      "5. Random Forest: Accuracy = 0.8750, F1 = 0.8889\n",
      "6. Logistic Regression: Accuracy = 0.8125, F1 = 0.8302\n",
      "\n",
      "Best model for Session 3 (XGBoost) saved as 'models/session_models/ssn3_xgboost.pkl'\n",
      "\n",
      "Session 3 training completed in 434.03 seconds\n",
      "\n",
      "================================================================================\n",
      "Training models for Session 4\n",
      "================================================================================\n",
      "Session 4 dataset shape: (480, 34137)\n",
      "Total records: 480\n",
      "Number of attacks: 240\n",
      "Number of normal operations: 240\n",
      "Attack percentage: 50.00%\n",
      "Training set shape: (432, 34134)\n",
      "Test set shape: (48, 34134)\n",
      "\n",
      "Applying variance threshold...\n",
      "Features after variance thresholding: 13297\n",
      "\n",
      "Selecting most informative features...\n",
      "Features after selection: 100\n",
      "\n",
      "Top 20 most informative features:\n",
      "ft922: 0.075509\n",
      "ft5167: 0.069465\n",
      "ft2383: 0.066897\n",
      "ft3095: 0.060995\n",
      "ft967: 0.060746\n",
      "ft1677: 0.059851\n",
      "ft8708: 0.053772\n",
      "ft8714: 0.052055\n",
      "ft9874: 0.050716\n",
      "ft5059: 0.049093\n",
      "ft8412: 0.047171\n",
      "ft77: 0.046694\n",
      "ft1624: 0.046394\n",
      "ft7254: 0.046309\n",
      "ft2385: 0.045568\n",
      "ft5636: 0.045396\n",
      "ft6585: 0.042523\n",
      "ft7355: 0.040571\n",
      "ft5838: 0.040557\n",
      "ft8046: 0.039847\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "1. Training Extra Trees Classifier for Session 4\n",
      "Best parameters: {'n_estimators': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 20, 'bootstrap': False}\n",
      "\n",
      "Extra Trees Results:\n",
      "Accuracy: 0.9583\n",
      "F1 Score: 0.9615\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 2 25]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        21\n",
      "           1       1.00      0.93      0.96        27\n",
      "\n",
      "    accuracy                           0.96        48\n",
      "   macro avg       0.96      0.96      0.96        48\n",
      "weighted avg       0.96      0.96      0.96        48\n",
      "\n",
      "ROC AUC: 0.9929\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_cos_10d: 0.114837\n",
      "fval_cos_10d_interaction: 0.102007\n",
      "ramp_constraint_active: 0.059248\n",
      "large_change_no_constraint_score: 0.057908\n",
      "change_std_ratio_no_constraint: 0.048321\n",
      "fval_diff_lag_1: 0.039581\n",
      "fval_cos_20d: 0.038626\n",
      "lower_limit_approach_rate: 0.036939\n",
      "large_change_no_constraint_magnitude: 0.036409\n",
      "fval_sin_20d: 0.035935\n",
      "\n",
      "2. Training Gradient Boosting Classifier for Session 4\n",
      "Best parameters: {'subsample': 1.0, 'n_estimators': 200, 'min_samples_split': 5, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_diff_lag_1: 0.246436\n",
      "fval_cos_10d_interaction: 0.130483\n",
      "fval_cos_10d: 0.112159\n",
      "fval_change_x_lower_limit_attack_score: 0.091829\n",
      "fval_change: 0.089002\n",
      "lower_limit_approach_rate: 0.083558\n",
      "fval_diff_lag_1_x_lower_limit_attack_score: 0.041936\n",
      "fval_diff_lag_1_x_weighted_attack_score: 0.038310\n",
      "fval_change_jerk: 0.027716\n",
      "fval_change_x_weighted_attack_score: 0.024201\n",
      "\n",
      "3. Training Random Forest Classifier for Session 4\n",
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': True}\n",
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.8958\n",
      "F1 Score: 0.9057\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 3 24]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        21\n",
      "           1       0.92      0.89      0.91        27\n",
      "\n",
      "    accuracy                           0.90        48\n",
      "   macro avg       0.89      0.90      0.89        48\n",
      "weighted avg       0.90      0.90      0.90        48\n",
      "\n",
      "ROC AUC: 0.9568\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_diff_lag_1: 0.093813\n",
      "fval_change: 0.084880\n",
      "fval_diff_lag_1_x_weighted_attack_score: 0.070844\n",
      "lower_limit_approach_rate: 0.062164\n",
      "fval_change_x_weighted_attack_score: 0.059546\n",
      "fval_change_x_lower_limit_attack_score: 0.053360\n",
      "fval_diff_lag_1_x_lower_limit_attack_score: 0.048039\n",
      "fval_diff_lag_1_x_large_change_no_constraint_score: 0.042301\n",
      "large_change_no_constraint_score: 0.035237\n",
      "fval_change_x_large_change_no_constraint_score: 0.034163\n",
      "\n",
      "4. Training XGBoost Classifier for Session 4\n",
      "Best parameters: {'subsample': 0.9, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "XGBoost Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Top 10 most important features:\n",
      "fval_diff_lag_1: 0.225574\n",
      "fval_change: 0.170327\n",
      "fval_change_x_lower_limit_attack_score: 0.165886\n",
      "ramp_constraint_active: 0.099158\n",
      "lower_limit_approach_rate: 0.073516\n",
      "ramp_constraint_active_x_weighted_attack_score: 0.046977\n",
      "change_std_ratio_no_constraint: 0.040166\n",
      "ramp_constraint_active_x_lower_limit_attack_score: 0.038402\n",
      "fval_cos_10d: 0.022445\n",
      "ft22485: 0.021706\n",
      "\n",
      "5. Training Logistic Regression for Session 4\n",
      "Best parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 1000, 'C': 1.0}\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.8542\n",
      "F1 Score: 0.8679\n",
      "Confusion Matrix:\n",
      "[[18  3]\n",
      " [ 4 23]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84        21\n",
      "           1       0.88      0.85      0.87        27\n",
      "\n",
      "    accuracy                           0.85        48\n",
      "   macro avg       0.85      0.85      0.85        48\n",
      "weighted avg       0.86      0.85      0.85        48\n",
      "\n",
      "ROC AUC: 0.8607\n",
      "\n",
      "6. Training Voting Classifier Ensemble for Session 4\n",
      "Top 3 models for ensemble: ['GradientBoosting', 'XGBoost', 'ExtraTrees']\n",
      "\n",
      "Voting Classifier Results:\n",
      "Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Confusion Matrix:\n",
      "[[21  0]\n",
      " [ 0 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        27\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "=== Model Performance Summary for Session 4 ===\n",
      "\n",
      "Models ranked by F1 score:\n",
      "1. Gradient Boosting: Accuracy = 1.0000, F1 = 1.0000\n",
      "2. XGBoost: Accuracy = 1.0000, F1 = 1.0000\n",
      "3. Voting Classifier: Accuracy = 1.0000, F1 = 1.0000\n",
      "4. Extra Trees: Accuracy = 0.9583, F1 = 0.9615\n",
      "5. Random Forest: Accuracy = 0.8958, F1 = 0.9057\n",
      "6. Logistic Regression: Accuracy = 0.8542, F1 = 0.8679\n",
      "\n",
      "Best model for Session 4 (Gradient Boosting) saved as 'models/session_models/ssn4_gradient_boosting.pkl'\n",
      "\n",
      "Session 4 training completed in 482.14 seconds\n",
      "\n",
      "=== Session Models Summary ===\n",
      "\n",
      "Model performance by session:\n",
      "+-------------------+-------------------+-------------------+-------------------+\n",
      "|Session            |Best Model         |Accuracy           |F1 Score           |\n",
      "+-------------------+-------------------+-------------------+-------------------+\n",
      "|1                  |Gradient Boosting  |1.0000             |1.0000             |\n",
      "|2                  |Gradient Boosting  |1.0000             |1.0000             |\n",
      "|3                  |Extra Trees        |0.8333             |0.8462             |\n",
      "|4                  |Gradient Boosting  |1.0000             |1.0000             |\n",
      "+-------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "Best performing session (Accuracy): Session 1 with 1.0000\n",
      "Worst performing session (Accuracy): Session 3 with 0.8333\n",
      "Best performing session (F1 Score): Session 1 with 1.0000\n",
      "Worst performing session (F1 Score): Session 3 with 0.8462\n",
      "\n",
      "=== Creating cross-session model comparison ===\n",
      "\n",
      "All session-specific models trained in 1978.44 seconds\n",
      "Results summary saved to 'session_figures/summaries/' directory\n",
      "Individual session figures saved to:\n",
      "  - 'session_figures/confusion_matrices/' for confusion matrices\n",
      "  - 'session_figures/roc_curves/' for ROC curves\n",
      "  - 'session_figures/feature_importance/' for feature importance plots\n",
      "All models saved to 'models/session_models/' directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the number of cores for parallel processing\n",
    "n_cores = max(1, os.cpu_count() // 2)\n",
    "print(f\"Using {n_cores} CPU cores for parallel processing\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('models/session_models', exist_ok=True)\n",
    "os.makedirs('session_figures', exist_ok=True)  # Separate folder for figures\n",
    "os.makedirs('session_figures/confusion_matrices', exist_ok=True)\n",
    "os.makedirs('session_figures/roc_curves', exist_ok=True)\n",
    "os.makedirs('session_figures/feature_importance', exist_ok=True)\n",
    "os.makedirs('session_figures/summaries', exist_ok=True)\n",
    "\n",
    "# Define a function to train models for a specific session\n",
    "def train_session_model(df, session_id, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Train all five models on data from a specific session with RandomizedSearchCV\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The full dataset\n",
    "    session_id : int\n",
    "        The session ID to filter by\n",
    "    test_size : float\n",
    "        The proportion of data to use for testing (default: 0.1 for 90-10 split)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training models for Session {session_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Filter data for this session\n",
    "    session_df = df[df['ssn'] == session_id].copy()\n",
    "    print(f\"Session {session_id} dataset shape: {session_df.shape}\")\n",
    "    \n",
    "    # Create binary target column if not already present\n",
    "    if 'attack' not in session_df.columns:\n",
    "        session_df['attack'] = (session_df['type_of_attack'] != 0).astype(int)  # 1 for attack, 0 for no attack\n",
    "    \n",
    "    # Display class distribution\n",
    "    attack_count = session_df['attack'].sum()\n",
    "    total_records = len(session_df)\n",
    "    normal_count = total_records - attack_count\n",
    "    print(f\"Total records: {total_records}\")\n",
    "    print(f\"Number of attacks: {attack_count}\")\n",
    "    print(f\"Number of normal operations: {normal_count}\")\n",
    "    print(f\"Attack percentage: {attack_count/total_records:.2%}\")\n",
    "    \n",
    "    # If either class has too few samples, notify but continue\n",
    "    if attack_count < 5 or normal_count < 5:\n",
    "        print(f\"WARNING: Session {session_id} has very few samples in one class.\")\n",
    "    \n",
    "    # Define feature columns, excluding the target and direct identifiers\n",
    "    feature_columns = [col for col in session_df.columns if col not in ['attack', 'type_of_attack', 'gen_attacked']]\n",
    "    \n",
    "    X = session_df[feature_columns]\n",
    "    y = session_df['attack']\n",
    "    \n",
    "    # Split dataset into training and testing sets (90-10 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=None)\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Feature selection and preprocessing pipeline\n",
    "    # 1. Remove low variance features\n",
    "    print(\"\\nApplying variance threshold...\")\n",
    "    variance_threshold = 0.01\n",
    "    selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    X_train_var = selector.fit_transform(X_train)\n",
    "    X_test_var = selector.transform(X_test)\n",
    "    print(f\"Features after variance thresholding: {X_train_var.shape[1]}\")\n",
    "    \n",
    "    # 2. Feature selection using mutual information\n",
    "    print(\"\\nSelecting most informative features...\")\n",
    "    select_k = SelectKBest(mutual_info_classif, k=min(100, X_train_var.shape[1]))\n",
    "    X_train_selected = select_k.fit_transform(X_train_var, y_train)\n",
    "    X_test_selected = select_k.transform(X_test_var)\n",
    "    print(f\"Features after selection: {X_train_selected.shape[1]}\")\n",
    "    \n",
    "    # Print top 20 feature names\n",
    "    selected_indices = select_k.get_support(indices=True)\n",
    "    original_indices = selector.get_support(indices=True)\n",
    "    selected_names = [X.columns[original_indices[i]] for i in selected_indices[:20]]\n",
    "    scores = select_k.scores_[selected_indices]\n",
    "    \n",
    "    print(\"\\nTop 20 most informative features:\")\n",
    "    for name, score in sorted(zip(selected_names, scores), key=lambda x: x[1], reverse=True)[:20]:\n",
    "        print(f\"{name}: {score:.6f}\")\n",
    "    \n",
    "    # 3. Scale features\n",
    "    print(\"\\nScaling features...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "    \n",
    "    # Define a function to evaluate models\n",
    "    def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "        # Create session-specific filename prefix\n",
    "        prefix = f\"ssn{session_id}_{model_name.replace(' ', '_').lower()}\"\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['Normal', 'Attack'],\n",
    "                    yticklabels=['Normal', 'Attack'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(f'Session {session_id} - Confusion Matrix - {model_name}')\n",
    "        plt.savefig(f'session_figures/confusion_matrices/{prefix}_confusion_matrix.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # ROC curve and AUC (if applicable)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            try:\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "                \n",
    "                # Plot ROC curve\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                        label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'Session {session_id} - ROC Curve - {model_name}')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.savefig(f'session_figures/roc_curves/{prefix}_roc_curve.png')\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not calculate ROC AUC: {e}\")\n",
    "                \n",
    "        # Feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_names = [X.columns[original_indices[i]] for i in selected_indices]\n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            # Plot top 20 feature importances\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.title(f'Session {session_id} - Top 20 Feature Importances - {model_name}')\n",
    "            n_features = min(20, len(importances))\n",
    "            plt.bar(range(n_features), \n",
    "                    importances[indices[:n_features]], align='center')\n",
    "            plt.xticks(range(n_features), \n",
    "                    [feature_names[i] for i in indices[:n_features]], rotation=90)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'session_figures/feature_importance/{prefix}_feature_importance.png')\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"\\nTop 10 most important features:\")\n",
    "            for i in range(min(10, len(importances))):\n",
    "                print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.6f}\")\n",
    "        \n",
    "        return model, accuracy, f1\n",
    "\n",
    "    # ====================== MODEL 1: EXTRA TREES CLASSIFIER ======================\n",
    "    print(\"\\n1. Training Extra Trees Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    et_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    et_base = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    et_cv = RandomizedSearchCV(\n",
    "        et_base, et_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    et_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    et_best = et_cv.best_estimator_\n",
    "    print(f\"Best parameters: {et_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    et_model, et_accuracy, et_f1 = evaluate_model(\n",
    "        et_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Extra Trees\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 2: GRADIENT BOOSTING ======================\n",
    "    print(\"\\n2. Training Gradient Boosting Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    gb_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    gb_base = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    gb_cv = RandomizedSearchCV(\n",
    "        gb_base, gb_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    gb_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    gb_best = gb_cv.best_estimator_\n",
    "    print(f\"Best parameters: {gb_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    gb_model, gb_accuracy, gb_f1 = evaluate_model(\n",
    "        gb_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Gradient Boosting\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 3: RANDOM FOREST ======================\n",
    "    print(\"\\n3. Training Random Forest Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    rf_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    rf_cv = RandomizedSearchCV(\n",
    "        rf_base, rf_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    rf_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    rf_best = rf_cv.best_estimator_\n",
    "    print(f\"Best parameters: {rf_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    rf_model, rf_accuracy, rf_f1 = evaluate_model(\n",
    "        rf_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Random Forest\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 4: XGBOOST ======================\n",
    "    print(\"\\n4. Training XGBoost Classifier for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3],\n",
    "        'gamma': [0, 0.1],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    xgb_base = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    xgb_cv = RandomizedSearchCV(\n",
    "        xgb_base, xgb_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    xgb_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    xgb_best = xgb_cv.best_estimator_\n",
    "    print(f\"Best parameters: {xgb_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    xgb_model, xgb_accuracy, xgb_f1 = evaluate_model(\n",
    "        xgb_best, X_train_scaled, X_test_scaled, y_train, y_test, \"XGBoost\"\n",
    "    )\n",
    "\n",
    "    # ====================== MODEL 5: LOGISTIC REGRESSION ======================\n",
    "    print(\"\\n5. Training Logistic Regression for Session\", session_id)\n",
    "\n",
    "    # Define parameter grid\n",
    "    lr_param_grid = {\n",
    "        'C': np.logspace(-3, 3, 7),\n",
    "        'penalty': ['l1', 'l2', None],\n",
    "        'solver': ['liblinear', 'saga', 'lbfgs'],\n",
    "        'max_iter': [1000, 2000]\n",
    "    }\n",
    "\n",
    "    # Create base model\n",
    "    lr_base = LogisticRegression(random_state=42)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    lr_cv = RandomizedSearchCV(\n",
    "        lr_base, lr_param_grid, n_iter=100, \n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='accuracy', n_jobs=n_cores, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    lr_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    lr_best = lr_cv.best_estimator_\n",
    "    print(f\"Best parameters: {lr_cv.best_params_}\")\n",
    "\n",
    "    # Evaluate\n",
    "    lr_model, lr_accuracy, lr_f1 = evaluate_model(\n",
    "        lr_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Logistic Regression\"\n",
    "    )\n",
    "\n",
    "    # ====================== ENSEMBLE MODEL: VOTING CLASSIFIER ======================\n",
    "    print(\"\\n6. Training Voting Classifier Ensemble for Session\", session_id)\n",
    "\n",
    "    # Create a dictionary of our models\n",
    "    models = {\n",
    "        'ExtraTrees': et_best,\n",
    "        'GradientBoosting': gb_best,\n",
    "        'RandomForest': rf_best,\n",
    "        'XGBoost': xgb_best,\n",
    "        'LogisticRegression': lr_best\n",
    "    }\n",
    "\n",
    "    # Calculate scores\n",
    "    model_scores = {\n",
    "        'ExtraTrees': et_f1,\n",
    "        'GradientBoosting': gb_f1,\n",
    "        'RandomForest': rf_f1,\n",
    "        'XGBoost': xgb_f1,\n",
    "        'LogisticRegression': lr_f1\n",
    "    }\n",
    "\n",
    "    # Sort by F1 score and select top 3\n",
    "    top_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(f\"Top 3 models for ensemble: {[model[0] for model in top_models]}\")\n",
    "\n",
    "    # Create voting classifier with top 3 models\n",
    "    estimators = [(name, models[name]) for name, _ in top_models]\n",
    "    voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "    # Evaluate\n",
    "    voting_model, voting_accuracy, voting_f1 = evaluate_model(\n",
    "        voting_clf, X_train_scaled, X_test_scaled, y_train, y_test, \"Voting Classifier\"\n",
    "    )\n",
    "\n",
    "    # ====================== SUMMARY ======================\n",
    "    print(\"\\n=== Model Performance Summary for Session\", session_id, \"===\")\n",
    "    models_summary = {\n",
    "        'Extra Trees': (et_accuracy, et_f1),\n",
    "        'Gradient Boosting': (gb_accuracy, gb_f1),\n",
    "        'Random Forest': (rf_accuracy, rf_f1),\n",
    "        'XGBoost': (xgb_accuracy, xgb_f1),\n",
    "        'Logistic Regression': (lr_accuracy, lr_f1),\n",
    "        'Voting Classifier': (voting_accuracy, voting_f1)\n",
    "    }\n",
    "\n",
    "    # Sort by F1 score\n",
    "    sorted_models = sorted(models_summary.items(), key=lambda x: x[1][1], reverse=True)\n",
    "\n",
    "    print(\"\\nModels ranked by F1 score:\")\n",
    "    for i, (model_name, (acc, f1)) in enumerate(sorted_models, 1):\n",
    "        print(f\"{i}. {model_name}: Accuracy = {acc:.4f}, F1 = {f1:.4f}\")\n",
    "\n",
    "    # Plot model comparison (both Accuracy and F1 Score)\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    model_names = [name for name, _ in sorted_models]\n",
    "    accuracies = [acc for _, (acc, _) in sorted_models]\n",
    "    f1_scores = [f1 for _, (_, f1) in sorted_models]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='skyblue')\n",
    "    rects2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='salmon')\n",
    "    \n",
    "    ax.set_xlabel('Models', fontsize=14)\n",
    "    ax.set_ylabel('Scores', fontsize=14)\n",
    "    ax.set_title(f'Session {session_id} - Model Performance Comparison', fontsize=16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=12)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_ylim(0, 1.1)  # Ensure consistent y-axis scale\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for rect in rects1:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    for rect in rects2:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Add a grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'session_figures/summaries/ssn{session_id}_model_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save the best model\n",
    "    best_model_name = sorted_models[0][0]\n",
    "    best_model = None\n",
    "\n",
    "    if best_model_name == 'Extra Trees':\n",
    "        best_model = et_model\n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        best_model = gb_model\n",
    "    elif best_model_name == 'Random Forest':\n",
    "        best_model = rf_model\n",
    "    elif best_model_name == 'XGBoost':\n",
    "        best_model = xgb_model\n",
    "    elif best_model_name == 'Logistic Regression':\n",
    "        best_model = lr_model\n",
    "    else:\n",
    "        best_model = voting_model\n",
    "\n",
    "    # Save model package\n",
    "    model_package = {\n",
    "        'variance_selector': selector,\n",
    "        'feature_selector': select_k,\n",
    "        'scaler': scaler,\n",
    "        'model': best_model,\n",
    "        'feature_columns': feature_columns,\n",
    "        'selected_indices': selected_indices,\n",
    "        'original_indices': original_indices,\n",
    "        'best_model_name': best_model_name,\n",
    "        'metrics': {'accuracy': sorted_models[0][1][0], 'f1': sorted_models[0][1][1]},\n",
    "        'all_models': {\n",
    "            'ExtraTrees': et_model,\n",
    "            'GradientBoosting': gb_model,\n",
    "            'RandomForest': rf_model,\n",
    "            'XGBoost': xgb_model,\n",
    "            'LogisticRegression': lr_model,\n",
    "            'VotingClassifier': voting_model\n",
    "        },\n",
    "        'all_metrics': models_summary\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/session_models/ssn{session_id}_{best_model_name.replace(' ', '_').lower()}.pkl\"\n",
    "    joblib.dump(model_package, model_path)\n",
    "    print(f\"\\nBest model for Session {session_id} ({best_model_name}) saved as '{model_path}'\")\n",
    "    \n",
    "    print(f\"\\nSession {session_id} training completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return model_package\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n=== Loading data ===\")\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(\"data/N300_G69_transposed.csv\", dtype={'ssn':'Int64', 'type_of_attack': 'Int64', 'gen_attacked': 'Int64'})\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    if 'attack' not in df.columns:\n",
    "        df['attack'] = (df['type_of_attack'] != 0).astype(int)  # 1 for attack, 0 for no attack\n",
    "    \n",
    "    # Perform domain-specific feature engineering (from the original)\n",
    "    print(\"\\n=== Performing domain-specific feature engineering ===\")\n",
    "    \n",
    "    # 1. Basic session and time features\n",
    "    df['day_in_ssn'] = df.groupby('ssn').cumcount() + 1  # Start at Day 1\n",
    "\n",
    "    # 2. Calculate OPF sensitivity features\n",
    "    # These help identify attacks on specific OPF parameters\n",
    "\n",
    "    # 2.1 Cost function sensitivity indicators\n",
    "    # Create features that might detect manipulation of generation costs (Type 4 attack)\n",
    "    df['fval_per_unit_load'] = df['fval'] / df.groupby('ssn')['fval'].transform('mean')\n",
    "    df['fval_normalized_by_ssn'] = df.groupby('ssn')['fval'].transform(\n",
    "        lambda x: (x - x.mean()) / (x.std() if x.std() != 0 else 1))\n",
    "\n",
    "    # 2.2 Ramp rate indicators \n",
    "    # Create features that might detect manipulation of ramp rates (Type 1 attack)\n",
    "    df['fval_change'] = df.groupby('ssn')['fval'].diff().fillna(0)\n",
    "    df['fval_change_rate'] = df['fval_change'] / df['fval'].shift(1).fillna(1)\n",
    "    df['fval_acceleration'] = df.groupby('ssn')['fval_change'].diff().fillna(0)\n",
    "\n",
    "    # Create rolling metrics to detect unusual ramp behavior\n",
    "    for window in [2, 3, 5]:\n",
    "        # Rolling standard deviation of changes (volatility)\n",
    "        df[f'fval_change_std_{window}d'] = df.groupby('ssn')['fval_change'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std().fillna(0))\n",
    "        \n",
    "        # Maximum change in the window\n",
    "        df[f'fval_max_change_{window}d'] = df.groupby('ssn')['fval_change'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).max().fillna(0))\n",
    "        \n",
    "        # Minimum change in the window\n",
    "        df[f'fval_min_change_{window}d'] = df.groupby('ssn')['fval_change'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).min().fillna(0))\n",
    "\n",
    "    # 2.3 Generation limit indicators\n",
    "    # Create features that might detect manipulation of upper/lower limits (Type 2 & 3 attacks)\n",
    "    df['fval_peak_ratio'] = df['fval'] / df.groupby('ssn')['fval'].transform('max')\n",
    "    df['fval_trough_ratio'] = df['fval'] / df.groupby('ssn')['fval'].transform('min')\n",
    "\n",
    "    # Use quantiles to detect limits being approached\n",
    "    df['fval_quantile_in_ssn'] = df.groupby('ssn')['fval'].transform(\n",
    "        lambda x: pd.qcut(x, q=10, labels=False, duplicates='drop').astype(float))\n",
    "\n",
    "    # 3. Create specialized detection features for each attack type\n",
    "    # These target the specific mechanisms of each attack type\n",
    "\n",
    "    # 3.1 Features for detecting ramp rate attacks (Type 1)\n",
    "    # Look for sudden discontinuities in how fast fval can change\n",
    "    df['ramp_constraint_active'] = (df['fval_change'].abs() > \n",
    "                                df.groupby('ssn')['fval_change'].transform('std')).astype(int)\n",
    "\n",
    "    # 3.2 Features for detecting generation limit attacks (Types 2 & 3)\n",
    "    # Look for values that should be infeasible under normal limits\n",
    "    df['upper_limit_proximity'] = 1 - (df['fval'] / df.groupby('ssn')['fval'].transform('max'))\n",
    "    df['lower_limit_proximity'] = (df['fval'] / df.groupby('ssn')['fval'].transform('min')) - 1\n",
    "\n",
    "    # 3.3 Features for detecting cost manipulation (Type 4)\n",
    "    # Look for cost-inefficient dispatches that shouldn't happen under normal cost functions\n",
    "    df['cost_efficiency'] = df['fval'] / df.groupby('ssn')['fval'].transform('mean')\n",
    "    df['cost_anomaly_score'] = df.groupby('ssn')['cost_efficiency'].transform(\n",
    "        lambda x: (x - x.mean()).abs() / (x.std() if x.std() != 0 else 1))\n",
    "\n",
    "    # 4. Inter-session comparison features\n",
    "    # These help identify if a session is behaving differently from others\n",
    "\n",
    "    # 4.1 Calculate average fval across all sessions for each day\n",
    "    day_avg = df.groupby('day_in_ssn')['fval'].transform('mean')\n",
    "    day_std = df.groupby('day_in_ssn')['fval'].transform('std')\n",
    "\n",
    "    # 4.2 Compare each session's values to the average across all sessions\n",
    "    df['fval_day_deviation'] = (df['fval'] - day_avg) / (day_std if day_std.any() != 0 else 1)\n",
    "    df['fval_day_pct_diff'] = (df['fval'] - day_avg) / day_avg\n",
    "\n",
    "    # 5. Statistical anomaly detection features\n",
    "    # These help identify outliers regardless of attack mechanism\n",
    "\n",
    "    # 5.1 Z-scores and modified Z-scores for more robust outlier detection\n",
    "    df['fval_median_dev'] = df.groupby('ssn')['fval'].transform(\n",
    "        lambda x: (x - x.median()) / (x.max() - x.min() if (x.max() - x.min()) != 0 else 1))\n",
    "\n",
    "    # 6. Cumulative features to detect subtle long-term manipulations\n",
    "    df['fval_cumsum'] = df.groupby('ssn')['fval'].cumsum()\n",
    "    df['fval_cummean'] = df.groupby('ssn')['fval'].expanding().mean().reset_index(level=0, drop=True)\n",
    "    df['fval_cum_deviation'] = df['fval'] - df['fval_cummean']\n",
    "\n",
    "    # 7. Trajectory features to detect changes in patterns\n",
    "    for lag in range(1, 4):\n",
    "        df[f'fval_lag_{lag}'] = df.groupby('ssn')['fval'].shift(lag).fillna(0)\n",
    "        df[f'fval_diff_lag_{lag}'] = df['fval'] - df[f'fval_lag_{lag}']\n",
    "            \n",
    "    # Clean any NaN values\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # ====================== ADVANCED FEATURE ENGINEERING ======================\n",
    "    # Apply the advanced feature engineering \n",
    "    from advanced_features import engineer_advanced_features\n",
    "    df = engineer_advanced_features(df)\n",
    "        \n",
    "    # Get unique session IDs\n",
    "    session_ids = df['ssn'].unique()\n",
    "    print(f\"Found {len(session_ids)} unique sessions: {session_ids}\")\n",
    "    \n",
    "    # Train models for each session\n",
    "    session_results = {}\n",
    "    \n",
    "    for session_id in session_ids:\n",
    "        model_package = train_session_model(df, session_id)\n",
    "        session_results[session_id] = model_package['metrics']['f1']\n",
    "    \n",
    "    # Create a summary of results across sessions\n",
    "    print(\"\\n=== Session Models Summary ===\")\n",
    "    print(\"\\nModel performance by session:\")\n",
    "    \n",
    "    # Table format for terminal display\n",
    "    headers = [\"Session\", \"Best Model\", \"Accuracy\", \"F1 Score\"]\n",
    "    rows = []\n",
    "    \n",
    "    # Dictionary to store accuracy and F1 scores for each session\n",
    "    accuracy_by_session = {}\n",
    "    f1_by_session = {}\n",
    "    best_models_by_session = {}\n",
    "   # Fixed version - only show the best model per session\n",
    "    for session_id in session_ids:\n",
    "        best_model_file = None\n",
    "        # Find the file that corresponds to the best model for this session\n",
    "        for file in os.listdir(\"models/session_models/\"):\n",
    "            if file.startswith(f\"ssn{session_id}_\") and not file.endswith(\"_noscale.pkl\"):\n",
    "                # If we already found a file for this session, we need to determine which is newer/correct\n",
    "                if best_model_file is None:\n",
    "                    best_model_file = file\n",
    "                \n",
    "        if best_model_file:\n",
    "            model_path = os.path.join(\"models/session_models/\", best_model_file)\n",
    "            model_package = joblib.load(model_path)\n",
    "            \n",
    "            accuracy = model_package['metrics']['accuracy']\n",
    "            f1 = model_package['metrics']['f1']\n",
    "            best_model = model_package['best_model_name']\n",
    "            \n",
    "            accuracy_by_session[session_id] = accuracy\n",
    "            f1_by_session[session_id] = f1\n",
    "            best_models_by_session[session_id] = best_model\n",
    "            \n",
    "            # Only add one row per session\n",
    "            rows.append([str(session_id), str(best_model), f\"{accuracy:.4f}\", f\"{f1:.4f}\"])\n",
    "    \n",
    "    # Print table\n",
    "    col_width = max(len(word) for row in [headers] + rows for word in row) + 2\n",
    "    print(\"+\" + \"+\".join([\"-\" * col_width for _ in headers]) + \"+\")\n",
    "    print(\"|\" + \"|\".join(word.ljust(col_width) for word in headers) + \"|\")\n",
    "    print(\"+\" + \"+\".join([\"-\" * col_width for _ in headers]) + \"+\")\n",
    "    for row in rows:\n",
    "        print(\"|\" + \"|\".join(str(word).ljust(col_width) for word in row) + \"|\")\n",
    "    print(\"+\" + \"+\".join([\"-\" * col_width for _ in headers]) + \"+\")\n",
    "    \n",
    "    # Identify best and worst performing sessions by both metrics\n",
    "    best_acc_session = max(accuracy_by_session.items(), key=lambda x: x[1])\n",
    "    worst_acc_session = min(accuracy_by_session.items(), key=lambda x: x[1])\n",
    "    best_f1_session = max(f1_by_session.items(), key=lambda x: x[1])\n",
    "    worst_f1_session = min(f1_by_session.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\nBest performing session (Accuracy): Session {best_acc_session[0]} with {best_acc_session[1]:.4f}\")\n",
    "    print(f\"Worst performing session (Accuracy): Session {worst_acc_session[0]} with {worst_acc_session[1]:.4f}\")\n",
    "    print(f\"Best performing session (F1 Score): Session {best_f1_session[0]} with {best_f1_session[1]:.4f}\")\n",
    "    print(f\"Worst performing session (F1 Score): Session {worst_f1_session[0]} with {worst_f1_session[1]:.4f}\")\n",
    "    \n",
    "    # Plot summary of scores by session\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    sessions = list(sorted(accuracy_by_session.keys()))\n",
    "    accuracies = [accuracy_by_session[s] for s in sessions]\n",
    "    f1_scores = [f1_by_session[s] for s in sessions]\n",
    "    \n",
    "    x = np.arange(len(sessions))\n",
    "    width = 0.35\n",
    "    \n",
    "    rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='skyblue')\n",
    "    rects2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='salmon')\n",
    "    \n",
    "    ax.set_xlabel('Session ID', fontsize=14)\n",
    "    ax.set_ylabel('Score', fontsize=14)\n",
    "    ax.set_title('Attack Detection Performance by Session', fontsize=16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'Session {s}' for s in sessions], fontsize=12)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for rect in rects1:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "                    \n",
    "    for rect in rects2:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/session_performance_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot best model by session\n",
    "    model_counts = {}\n",
    "    for session_id in session_ids:\n",
    "        files = os.listdir(\"models/session_models/\")\n",
    "        for file in files:\n",
    "            if file.startswith(f\"ssn{session_id}_\"):\n",
    "                model_name = file.replace(f\"ssn{session_id}_\", \"\").replace(\".pkl\", \"\").replace(\"_\", \" \").title()\n",
    "                if model_name not in model_counts:\n",
    "                    model_counts[model_name] = 0\n",
    "                model_counts[model_name] += 1\n",
    "    \n",
    "    # Create a bar chart of best model types\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    models = list(model_counts.keys())\n",
    "    counts = [model_counts[m] for m in models]\n",
    "    \n",
    "    plt.bar(models, counts, color='lightgreen')\n",
    "    plt.xlabel('Model Type', fontsize=14)\n",
    "    plt.ylabel('Number of Sessions', fontsize=14)\n",
    "    plt.title('Best Model Type by Number of Sessions', fontsize=16)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for i, v in enumerate(counts):\n",
    "        plt.text(i, v + 0.1, str(v), ha='center', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/best_model_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create comparison table for all models across all sessions\n",
    "    print(\"\\n=== Creating cross-session model comparison ===\")\n",
    "    \n",
    "    # Dictionary to store F1 scores for each model type across sessions\n",
    "    model_performance = {\n",
    "        'Extra Trees': {},\n",
    "        'Gradient Boosting': {},\n",
    "        'Random Forest': {},\n",
    "        'XGBoost': {},\n",
    "        'Logistic Regression': {},\n",
    "        'Voting Classifier': {}\n",
    "    }\n",
    "    \n",
    "    # Load model packages to get performance metrics\n",
    "    for session_id in session_ids:\n",
    "        for file in os.listdir(\"models/session_models/\"):\n",
    "            if file.startswith(f\"ssn{session_id}_\"):\n",
    "                model_path = os.path.join(\"models/session_models/\", file)\n",
    "                model_package = joblib.load(model_path)\n",
    "                \n",
    "                # Extract all model metrics\n",
    "                for model_name, (acc, f1) in model_package['all_metrics'].items():\n",
    "                    model_performance[model_name][session_id] = f1\n",
    "    \n",
    "    # Create a heatmap of model performance across sessions\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Initialize data structures for both metrics\n",
    "    accuracy_heatmap_data = []\n",
    "    f1_heatmap_data = []\n",
    "    model_names = list(model_performance.keys())\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        # Collect accuracy scores\n",
    "        accuracy_scores = []\n",
    "        f1_scores = []\n",
    "        for session_id in session_ids:\n",
    "            # For each session, load the model package and get metrics for all models\n",
    "            found = False\n",
    "            for file in os.listdir(\"models/session_models/\"):\n",
    "                if file.startswith(f\"ssn{session_id}_\"):\n",
    "                    model_path = os.path.join(\"models/session_models/\", file)\n",
    "                    model_package = joblib.load(model_path)\n",
    "                    \n",
    "                    # Extract metrics for this model\n",
    "                    if model_name in model_package['all_metrics']:\n",
    "                        acc, f1 = model_package['all_metrics'][model_name]\n",
    "                        accuracy_scores.append(acc)\n",
    "                        f1_scores.append(f1)\n",
    "                        found = True\n",
    "                        break\n",
    "            \n",
    "            if not found:\n",
    "                accuracy_scores.append(0)\n",
    "                f1_scores.append(0)\n",
    "                \n",
    "        accuracy_heatmap_data.append(accuracy_scores)\n",
    "        f1_heatmap_data.append(f1_scores)\n",
    "    \n",
    "    # Create accuracy heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(accuracy_heatmap_data, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", \n",
    "                xticklabels=[f\"Session {s}\" for s in session_ids],\n",
    "                yticklabels=model_names,\n",
    "                vmin=0, vmax=1)\n",
    "    \n",
    "    plt.title('Accuracy by Model and Session', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/model_session_accuracy_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create F1 score heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(f1_heatmap_data, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", \n",
    "                xticklabels=[f\"Session {s}\" for s in session_ids],\n",
    "                yticklabels=model_names,\n",
    "                vmin=0, vmax=1)\n",
    "    \n",
    "    plt.title('F1 Score by Model and Session', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('session_figures/summaries/model_session_f1_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create attack type distribution visualization\n",
    "    if 'type_of_attack' in df.columns:\n",
    "        attack_types = df[df['attack'] == 1]['type_of_attack'].value_counts()\n",
    "        \n",
    "        # Map attack types to descriptive names\n",
    "        attack_type_names = {\n",
    "            0: \"Normal\",\n",
    "            1: \"Ramp Rate Attack\",\n",
    "            2: \"Upper Limit Attack\",\n",
    "            3: \"Lower Limit Attack\",\n",
    "            4: \"Generation Cost Attack\"\n",
    "        }\n",
    "        \n",
    "        # Create a separate dataset for attack type distribution by session\n",
    "        attack_by_session = {}\n",
    "        for session_id in session_ids:\n",
    "            session_attacks = df[(df['ssn'] == session_id) & (df['attack'] == 1)]['type_of_attack'].value_counts()\n",
    "            attack_by_session[session_id] = {attack_type_names.get(k, f\"Type {k}\"): v \n",
    "                                           for k, v in session_attacks.items()}\n",
    "        \n",
    "        # Prepare data for stacked bar chart\n",
    "        attack_types_data = []\n",
    "        for attack_type in range(1, 5):  # Attack types 1-4\n",
    "            type_data = []\n",
    "            for session_id in session_ids:\n",
    "                if attack_type_names.get(attack_type) in attack_by_session[session_id]:\n",
    "                    type_data.append(attack_by_session[session_id][attack_type_names.get(attack_type)])\n",
    "                else:\n",
    "                    type_data.append(0)\n",
    "            attack_types_data.append(type_data)\n",
    "        \n",
    "        # Create stacked bar chart\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        bottom = np.zeros(len(session_ids))\n",
    "        \n",
    "        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "        \n",
    "        for i, attack_data in enumerate(attack_types_data):\n",
    "            plt.bar([f\"Session {s}\" for s in session_ids], attack_data, bottom=bottom, \n",
    "                   label=attack_type_names.get(i+1), color=colors[i])\n",
    "            bottom += attack_data\n",
    "        \n",
    "        plt.xlabel('Session', fontsize=14)\n",
    "        plt.ylabel('Number of Attacks', fontsize=14)\n",
    "        plt.title('Attack Type Distribution by Session', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.xticks(rotation=0, fontsize=12)\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('session_figures/summaries/attack_type_distribution_by_session.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Create pie charts for each session's attack distribution\n",
    "        for session_id in session_ids:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            session_data = df[(df['ssn'] == session_id) & (df['attack'] == 1)]\n",
    "            \n",
    "            if len(session_data) > 0:\n",
    "                attack_counts = session_data['type_of_attack'].value_counts()\n",
    "                labels = [attack_type_names.get(i, f\"Type {i}\") for i in attack_counts.index]\n",
    "                \n",
    "                plt.pie(attack_counts, labels=labels, autopct='%1.1f%%', \n",
    "                       startangle=90, colors=colors, wedgeprops={'edgecolor': 'black'})\n",
    "                plt.axis('equal')\n",
    "                plt.title(f'Session {session_id} - Attack Type Distribution', fontsize=16)\n",
    "                plt.savefig(f'session_figures/summaries/session{session_id}_attack_distribution_pie.png')\n",
    "                plt.close()\n",
    "    \n",
    "    print(f\"\\nAll session-specific models trained in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Results summary saved to 'session_figures/summaries/' directory\")\n",
    "    print(f\"Individual session figures saved to:\")\n",
    "    print(f\"  - 'session_figures/confusion_matrices/' for confusion matrices\")\n",
    "    print(f\"  - 'session_figures/roc_curves/' for ROC curves\")\n",
    "    print(f\"  - 'session_figures/feature_importance/' for feature importance plots\")\n",
    "    print(f\"All models saved to 'models/session_models/' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
